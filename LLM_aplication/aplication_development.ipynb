{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TJhon/lanchain_curso/blob/day3/LLM_aplication/aplication_development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osNe4OoVGIv-"
      },
      "source": [
        "%pip install openai langchain tiktoken wikipedia langchain-experimental langchainhub docarray -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "ZUrxjvLgG7Py"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpKt18dUGIwA"
      },
      "source": [
        "# Application Development\n",
        "\n",
        "## Quick Overview\n",
        "\n",
        "### Prompts\n",
        "\n",
        "Prompts are short instructions or phrases used to guide a language model like a Machine Learning Language Model (LLM) on what type of text to generate. These instructions can be as simple as a word or phrase, or they can be complete paragraphs, depending on the desired response type and the specific model being used.\n",
        "\n",
        "### Langchain\n",
        "\n",
        "`LangChain` is a framework for developing applications powered by language models. It enables applications that:\n",
        "\n",
        "- Are context-aware: Connect a language model to sources of context (prompt instructions, few-shot examples, content to ground its response in, etc.).\n",
        "- Reason: Rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.).\n",
        "\n",
        "\n",
        "\n",
        "<!--\n",
        "### Langchain: Question and Answer with DocArrayInMemorySearch\n",
        "\n",
        "Anteriormente se uso Chroma para almacenar la base de datos, ahora se usara `DocArrayInMemorySearch` para hacer las consultas, el procedimiento es similar -->\n",
        "\n",
        "\n",
        "<!-- ####\n",
        "\n",
        "- Question and Answer\n",
        "  - leer un documento\n",
        "  - guardar el documento en una base de datos pasando previamente por un modelo de embeddings\n",
        "  - hacer las consultas\n",
        "  - la base de datos retorna una lista de elementos que segun la coincidencia de vectoores, son importantes para responder la consulta\n",
        "  - los documentos se pasan por un motor de llm para sintetizar y generar una respuesta adecuada  -->\n",
        "\n",
        "\n",
        "\n",
        "## Model Parsel\n",
        "\n",
        "### Chat API: Open AI\n",
        "\n",
        "- [Open ai version 1.0.0](https://github.com/openai/openai-python/discussions/742)\n",
        "\n",
        "On the internet, several tutorials can be found on how to use the OpenAI API, but if these tutorials are dated before November, they might be outdated. This is because installing `pip install openai` will provide a version of 1.x.x, whereas tutorials before that date were working with the beta version.\n",
        "\n",
        "For example, to access the OpenAI chat, note in the following example that the new model of creation is based on creating an instance of the `OpenAI()` module instead of having it globally, and to access the methods we switch from `openai.ChatCompletion` to `OpenAI().chat.completions`:\n",
        "\n",
        "\n",
        "```python\n",
        "# old\n",
        "import openai\n",
        "\n",
        "completion = openai.ChatCompletion.acreate(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
        "\n",
        "# new\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "completion = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
        "```\n",
        "\n",
        "Regarding responses, it relies on [`pydantic models`](https://docs.pydantic.dev/latest/concepts/models/), meaning that dictionaries are no longer used, but rather attributes. However, `pydantic` allows conversion to a dictionary with `model.dict()`:\n",
        "\n",
        "```python\n",
        "\n",
        "# old\n",
        "import json\n",
        "import openai\n",
        "\n",
        "completion = openai.Completion.create(model='gpt-3.5-turbo', messages=[{}])\n",
        "print(completion['choices'][0]['text']) # /// old\n",
        "print(completion.get('usage')) #  /// old\n",
        "print(json.dumps(completion, indent=2))\n",
        "\n",
        "# new\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "completion = client.completions.create(model='gpt-3.5-turbo', messages=[{}])\n",
        "print(completion.choices[0].message.content)  #/// new\n",
        "```\n",
        "\n",
        "First, we need to call the class, create the OpenAI client, and pass our api_key as a parameter. However, if we are using environment files `.env`, we only need to have the variable name correctly set, in this case `OPENAI_API_KEY`. In which we can define our api_key.\n",
        "\n",
        "![](https://imgs.search.brave.com/j6GBXNcpKlQxBOvfNFyojLGZjzHPUAYi71MoT5kmFes/rs:fit:860:0:0/g:ce/aHR0cHM6Ly9sYWJz/LnRoaW5rdGVjdHVy/ZS5jb20vc3RvcmFn/ZS9pbWFnZS01LnBu/Zw)\n",
        "\n",
        "To access the variable, we must run the following code and call our environment variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_ixjlW7GIwC"
      },
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = api_key"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtvUkn27GIwC"
      },
      "source": [
        "To directly access our response, a function is created which takes the question (or prompt) and the model we want to use as parameters, emphasizing how responses from pydantic are now used. Therefore, we must access the response with the attributes, in this case `completion.choices[0].message.content`. Let's try with a general question and a prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUoq5gLvGIwC",
        "outputId": "a01f0231-b05d-490b-9863-c150ac8914ac"
      },
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How do I output all files in a directory using Python?\",\n",
        "        },\n",
        "    ],\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To output all files in a directory using Python, you can use the `os` module. Here's an example:\n",
            "\n",
            "```python\n",
            "import os\n",
            "\n",
            "def output_files(directory):\n",
            "    for file in os.listdir(directory):\n",
            "        if os.path.isfile(os.path.join(directory, file)):\n",
            "            print(file)\n",
            "\n",
            "# Provide the directory path here\n",
            "directory_path = \"/path/to/directory\"\n",
            "output_files(directory_path)\n",
            "```\n",
            "\n",
            "1. First, import the `os` module.\n",
            "2. Define a function `output_files` that takes the directory path as a parameter.\n",
            "3. Use `os.listdir(directory)` to get a list of all files and directories in the specified directory.\n",
            "4. Loop through each item in the list.\n",
            "5. Use `os.path.isfile(os.path.join(directory, file))` to check if the item is a file (not a directory).\n",
            "6. If it is a file, print its name using `print(file)`.\n",
            "\n",
            "Make sure to replace `\"/path/to/directory\"` with the actual directory path you want to output the files from.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j9ppV0MGIwC"
      },
      "source": [
        "### Open AI and LangChain\n",
        "\n",
        "Fortunately, LangChain updates its code as its dependencies do, so in the latest versions of LangChain, this migration is already implemented.\n",
        "\n",
        "To use this way of calling OpenAI, we can replicate the previous example with prompts. First, we call the `ChatOpenAI` module and define the prompt with the variables we want, and save it in the `message` variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWyAoJY0GIwD",
        "outputId": "5b9ef514-c39a-4d2b-a304-be730c94ed0f"
      },
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "llm_model = 'gpt-3.5-turbo'\n",
        "\n",
        "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "# chat # callable([list])\n",
        "\n",
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "variable1 = \"\"\"American English \\\n",
        "in a calm and respectful tone\n",
        "\"\"\"\n",
        "variable2 = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse, \\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\"\n",
        "\n",
        "message = prompt_template.format_messages(style=variable1, text=variable2)\n",
        "print(message)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HumanMessage(content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jFk6J4rGIwD"
      },
      "source": [
        "Finally, we get the response."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G4LYNM5GIwD",
        "outputId": "de720baa-604a-435d-b8aa-7b71dcc467c2"
      },
      "source": [
        "response = chat(message)\n",
        "print(response.content)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! And to make things even worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, my friend!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O6Uw1u_GIwD"
      },
      "source": [
        "## Output Parser\n",
        "\n",
        "In some cases, you may want the response type to have a certain structure, such as JSON. However, when we obtain the response from LangChain, we notice that the result is a string and we cannot access its elements.\n",
        "\n",
        "Fortunately, we can convert the output string to a Python dictionary. First, we import `ResponseSchema` and `StructuredOutputParser`. Then, we create the elements that we want to extract with a description so that the model knows what to parse and format as required. Finally, we pass this through the format instructions within the initial prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nHmSp3VGIwE",
        "outputId": "3df369ac-c398-4ba6-de3e-15b5563007a9"
      },
      "source": [
        "# Base, repeat steps\n",
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\"\n",
        "\n",
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "\n",
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "response = chat(messages)\n",
        "\n",
        "print('Content', response.content)\n",
        "print(\"type\", type(response.content))\n",
        "# It's just a String"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content {\n",
            "  \"gift\": false,\n",
            "  \"delivery_days\": 2,\n",
            "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
            "}\n",
            "type <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3SYNP_0GIwE"
      },
      "source": [
        "Parse Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyp5wHj4GIwE",
        "outputId": "7ff32d92-2aa6-46ba-c5c8-d8a056580e15"
      },
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "# Define the Schemas, with descriptions\n",
        "gift_schema = ResponseSchema(name=\"gift\",\n",
        "                             description=\"Was the item purchased\\\n",
        "                             as a gift for someone else? \\\n",
        "                             Answer True if yes,\\\n",
        "                             False if not or unknown.\")\n",
        "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
        "                                      description=\"How many days\\\n",
        "                                      did it take for the product\\\n",
        "                                      to arrive? If this \\\n",
        "                                      information is not found,\\\n",
        "                                      output -1.\")\n",
        "price_value_schema = ResponseSchema(name=\"price_value\",\n",
        "                                    description=\"Extract any\\\n",
        "                                    sentences about the value or \\\n",
        "                                    price, and output them as a \\\n",
        "                                    comma separated Python list.\")\n",
        "\n",
        "response_schemas = [gift_schema,\n",
        "                    delivery_days_schema,\n",
        "                    price_value_schema]\n",
        "\n",
        "# Define the method into a `output` parser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
            "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
            "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uKMHsFeGIwE",
        "outputId": "669436e1-2700-403b-dc32-9f396a7059f9"
      },
      "source": [
        "# Run LLm\n",
        "review_template_2 = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product\\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "text: {text}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
        "\n",
        "messages = prompt.format_messages(text=customer_review,\n",
        "                                format_instructions=format_instructions)\n",
        "response = chat(messages)\n",
        "print(\"content\", response.content)\n",
        "print(\"type\", type(response.content))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content ```json\n",
            "{\n",
            "\t\"gift\": false,\n",
            "\t\"delivery_days\": \"2\",\n",
            "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
            "}\n",
            "```\n",
            "type <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzajxjrhGIwE"
      },
      "source": [
        "When the model is run, the response obtained is as per the format instructions provided. We then pass it through the `parse` method of the content, which returns the Python dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M12M0GAnGIwE",
        "outputId": "1364770c-aa15-4fb3-c6f9-1413feffd2dd"
      },
      "source": [
        "# Get dictionary\n",
        "\n",
        "result_dict = output_parser.parse(response.content)\n",
        "result_dict"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gift': False,\n",
              " 'delivery_days': '2',\n",
              " 'price_value': \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict.get(\"gift\")"
      ],
      "metadata": {
        "id": "3djZxoo8HU53",
        "outputId": "ff596013-bb1d-4967-d75e-99853544d1cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCu0dmWZGIwE"
      },
      "source": [
        "## LangChain Memory\n",
        "\n",
        "When interacting with `ChatGPT`, the responses from one interaction to the next are often related because `ChatGPT` contextualizes its responses, referencing previous inputs and outputs to formulate the latest response.\n",
        "\n",
        "![](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)\n",
        "\n",
        "To simulate this scenario with LangChain, we utilize four types of memory:\n",
        "\n",
        "- ConversationBufferMemory\n",
        "- ConversationBufferWindowMemory\n",
        "- ConversationTokenBufferMemory\n",
        "- ConversationSummaryMemory\n",
        "<!-- - **ConversationBufferMemory**: Records and manages previous parts of a conversation, allowing language models to maintain conversational context. It's crucial for applications like chatbots to ensure a seamless conversational flow.\n",
        "\n",
        "- **ConversationBufferWindowMemory**: Retains a window of recent conversation exchanges, limiting memory growth. Useful for keeping track of recent terms without accumulating excessive memory.\n",
        "\n",
        "- **ConversationTokenBufferMemory**: Manages conversation tokens, controlling memory based on token limits. Helps manage costs associated with token-based pricing for language model calls.\n",
        "\n",
        "- **ConversationSummaryMemory**: Summarizes conversation history using an LLM, providing a concise overview of past exchanges. Useful for condensing lengthy conversations into manageable summaries, aiding memory storage efficiency. -->\n",
        "\n",
        "To use these methods, we can refer to the following example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii0ei0JIGIwF"
      },
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "llm_model = 'gpt-3.5-turbo'\n",
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS6m7VSAGIwF"
      },
      "source": [
        "```python\n",
        "# Reference only\n",
        "from langchain.memory import Conversation{Method}Memory as Memory\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.0, model='gpt-3.5-turbo')\n",
        "memory = Memory()\n",
        "conversation = ConversationChains(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "conversation.predict(input='Question')  # Add memory (input, output)\n",
        "memory.load_memory_variables({})  # Returns memory (history)\n",
        "memory.save_context({\"input\": \"user_content\", \"output\": \"AI response\"})  # Add manual memory\n",
        "```\n",
        "\n",
        "\n",
        "### ConversationBufferMemory\n",
        "\n",
        "Using this in a chain (setting verbose=True so we can see the prompt)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "MZUzllMbGIwF",
        "outputId": "c77139f1-4e14-4dcf-c40e-1c427f55d7cd"
      },
      "source": [
        "from langchain.memory import ConversationBufferMemory as Memory\n",
        "\n",
        "memory = Memory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "conversation.predict(input=\"Hi there!\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "qFHBSgxwGIwF",
        "outputId": "c2aa71ea-e134-4145-80f4-817067709040"
      },
      "source": [
        "conversation.predict(input=\"Tell me about yourself.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI: Hello! How can I assist you today?\n",
            "Human: Tell me about yourself.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am an AI language model developed by OpenAI. I have been trained on a wide range of data sources, including books, articles, and websites, to generate human-like responses to text inputs. My purpose is to assist users like you by providing information, answering questions, and engaging in friendly conversations. Is there anything specific you would like to know about me?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRXM-3tAGIwF",
        "outputId": "84e38606-803d-41c6-8814-723aa830b758"
      },
      "source": [
        "memory.load_memory_variables({})"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Hi there!\\nAI: Hello! How can I assist you today?\\nHuman: Tell me about yourself.\\nAI: I am an AI language model developed by OpenAI. I have been trained on a wide range of data sources, including books, articles, and websites, to generate human-like responses to text inputs. My purpose is to assist users like you by providing information, answering questions, and engaging in friendly conversations. Is there anything specific you would like to know about me?'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oNfNqNdGIwF"
      },
      "source": [
        "### ConversationBufferWindowMemory\n",
        "\n",
        "`ConversationBufferWindowMemory` keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgAAhsJQGIwF",
        "outputId": "ae121b3e-8077-432a-f98a-f66d2b6d115d"
      },
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory as Memory\n",
        "\n",
        "memory = Memory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
        "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: hi\\nAI: whats up\\nHuman: not much you\\nAI: not much'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmrzBxK8GIwF"
      },
      "source": [
        "### ConversationTokenBufferMemory\n",
        "\n",
        "`ConversationTokenBufferMemory` keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fC5CTZqGIwF",
        "outputId": "cdd8116a-8b57-4095-ba8d-7882d74d3b9c"
      },
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "\n",
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=70) #test: max_token_limit -> 50\n",
        "conversation = ConversationChain(\n",
        "    llm = llm, memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "memory.save_context({\"input\": \"Hi, my name is 'SomeOne'\"},\n",
        "                    {\"output\": \"Hi SomeOne\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is 'SomeOne'\\nAI: Hi SomeOne\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Wha's my name?\")"
      ],
      "metadata": {
        "id": "jZwh-aLxIel7",
        "outputId": "e091af7a-0e7c-4121-d4b7-9e3c20766874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is 'SomeOne'\n",
            "AI: Hi SomeOne\n",
            "Human: Backpropagation is what?\n",
            "AI: Beautiful!\n",
            "Human: Chatbots are what?\n",
            "AI: Charming!\n",
            "Human: Wha's my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is SomeOne.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNv9WEKuGIwF"
      },
      "source": [
        "### ConversationSummaryMemory\n",
        "\n",
        "`ConversationSummaryBufferMemory` combines the two ideas. It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both. It uses token length rather than number of interactions to determine when to flush interactions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7RejHHeGIwG",
        "outputId": "66288e69-37f1-4075-c070-f299b858a1de"
      },
      "source": [
        "from langchain.memory import ConversationSummaryMemory as Memory\n",
        "\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory = Memory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
        "memory.save_context(\n",
        "    {\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"}\n",
        ")\n",
        "\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
        "\n",
        "conversation.predict(input=\"What would be a good demo to show?\")\n",
        "memory.load_memory_variables({})"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'The human greets the AI and the AI asks what\\'s up. The human replies that they are just hanging. The AI responds with a simple \"Cool.\" The human then asks about the schedule for the day. The AI informs the human about a meeting at 8am with the product team, the need to prepare a PowerPoint presentation, and time to work on the LangChain project. At noon, there is a lunch meeting with a customer who is driving from over an hour away to discuss the latest in AI, and the AI advises the human to bring their laptop to show the latest LLM demo. The human asks what would be a good demo to show, and the AI suggests showcasing the Language Model API\\'s capabilities in generating coherent and contextually relevant responses to different prompts. It also mentions the API\\'s ability to understand and respond to complex queries, making it valuable for applications like chatbots, virtual assistants, and content generation.'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3241o63SGIwG"
      },
      "source": [
        "## LangChain: Chains\n",
        "\n",
        "Previously, we used chains, but in a quick manner to obtain a response from the LLM model.\n",
        "\n",
        "Chains in LangChain encompass sequences of function calls, whether to an LLM, a tool, or a data preprocessing step. LCEL (LangChain Execution Language) is the primary method for constructing these chains, offering both custom and off-the-shelf options.\n",
        "\n",
        "Combine multiple chains where the output of one chain is the input of the next chain.\n",
        "\n",
        "- **LLMchain**: It's just the combination of the LLM and the prompt (simple chain).\n",
        "\n",
        "\n",
        "- **Simple Sequential Chain**: A chain where one chain's output is another chain's input, resulting in a final output.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*sYdb7ca9vcmDV0gOiNaruQ.png\" width=\"600px\" height=\"300px\">\n",
        "\n",
        "- **Sequential Chain**: Similar to the simple sequential chain but allows multiple input variables for each step, useful for more complex downstream chains.\n",
        "\n",
        "![](https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/7_Bpn6EWD-thumbnail_webp-600x300.webp)\n",
        "\n",
        "- **Router Chain**: Utilizes an LLM to route between potential options, deciding which sub-chain to pass input to based on input characteristics.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:700/1*0TDSAfaL2Q46TnFWkQTagg.jpeg\" width=\"600px\" height=\"300px\">\n",
        "\n",
        "### Usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq_UoBm7GIwG"
      },
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate as CPrompt\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm_model = 'gpt-3.5-turbo'\n",
        "llm = ChatOpenAI(temperature=0.9, model=llm_model)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ptfs5VEGIwG"
      },
      "source": [
        "### LLM chain:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-B0vMdvgGIwG",
        "outputId": "0feb0bde-b99a-46c9-8892-a4f3fe7620c1"
      },
      "source": [
        "prompt = CPrompt.from_template(\"What is the best name to describe \\\n",
        "    a company that makes {product}?\")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "input_ = \"Magic wands in the Harry Potter universe\"\n",
        "chain.run(input_)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'EnchantiWands'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_BHgIC7GIwG"
      },
      "source": [
        "### SimpleSequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "Ag3_FurpGIwG",
        "outputId": "65c72970-1673-478f-ad7d-455552f1cbe9"
      },
      "source": [
        "first_prompt = CPrompt.from_template(\n",
        "    \"What is the best name to describe \\\n",
        "    a company that makes {product}?\"\n",
        ")\n",
        "first_chain = LLMChain(llm=llm, prompt=first_prompt)\n",
        "second_prompt = CPrompt.from_template(\n",
        "    \"Write a 20 words description for the following \\\n",
        "    company:{company_name}\"\n",
        ")\n",
        "second_chain = LLMChain(llm=llm, prompt=second_prompt)\n",
        "\n",
        "simple_chain = SimpleSequentialChain(\n",
        "    chains=[first_chain, second_chain],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "input_1 = \"Magic wands in the Harry Potter universe\"\n",
        "\n",
        "simple_chain.run(input_1)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCharmed Creations\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mCharmed Creations is a unique jewelry company specializing in customizable charm bracelets and personalized accessories for every occasion.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Charmed Creations is a unique jewelry company specializing in customizable charm bracelets and personalized accessories for every occasion.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDWJ5EeAGIwG"
      },
      "source": [
        "### SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPHSsn34GIwG"
      },
      "source": [
        "from langchain.chains import SequentialChain\n",
        "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
        "\n",
        "# prompt template 1: translate to english\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the following review to english:\"\n",
        "    \"\\n\\n{Review}\"\n",
        ")\n",
        "# chain 1: input= Review and output= English_Review\n",
        "chain_one = LLMChain(\n",
        "    llm=llm, prompt=first_prompt,\n",
        "    output_key=\"English_Review\"\n",
        "    )\n",
        "\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following review in 1 sentence:\"\n",
        "    \"\\n\\n{English_Review}\"\n",
        ")\n",
        "# chain 2: input= English_Review and output= summary\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt,\n",
        "                     output_key=\"summary\"\n",
        "                    )\n",
        "# prompt template 3: translate to english\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What language is the following review:\\n\\n{Review}\"\n",
        ")\n",
        "# chain 3: input= Review and output= language\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                       output_key=\"language\"\n",
        "                      )\n",
        "\n",
        "# prompt template 4: follow up message\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following \"\n",
        "    \"summary in the specified language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
        ")\n",
        "# chain 4: input= summary, language and output= followup_message\n",
        "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
        "                      output_key=\"followup_message\"\n",
        "                     )\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7F2cBqOGIwG",
        "outputId": "f78d0725-7c10-4595-d661-1a5c2cfb49af"
      },
      "source": [
        "review = \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
        "\n",
        "# overall_chain: input= Review\n",
        "# and output= English_Review,summary, followup_message\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "overall_chain(review)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
              " 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in stores and the taste is much better...\\nOld batch or counterfeit!?\",\n",
              " 'summary': 'The reviewer is disappointed with the taste and quality of the product, suspecting that it may be either an old batch or a counterfeit.',\n",
              " 'followup_message': \"Réponse de suivi :\\n\\nCher(e) client(e),\\n\\nNous vous remercions d'avoir partagé votre avis sur notre produit. Nous sommes désolés d'apprendre que vous avez été déçu(e) par le goût et la qualité du produit. Votre satisfaction est notre priorité absolue et nous nous engageons à fournir des produits de la plus haute qualité.\\n\\nIl est important de noter que nous prenons les préoccupations de nos clients très au sérieux. Nous souhaitons enquêter sur ce problème afin de comprendre ce qui s'est passé. Pourriez-vous nous fournir plus de détails sur votre expérience, tels que la date d'achat, le code de lot ou toute autre information pertinente ? Cela nous aiderait énormément à résoudre ce problème.\\n\\nNous tenons à vous assurer que nous ne proposons pas de produits contrefaits et que tous nos produits sont fabriqués selon les normes les plus strictes. Si vous avez des soupçons quant à l'authenticité du produit, nous vous encourageons à contacter notre service clientèle pour nous en informer. Nous serions ravis de fournir toute assistance nécessaire pour résoudre ce problème.\\n\\nEncore une fois, nous sommes désolés de ne pas avoir répondu à vos attentes et nous vous remercions d'avoir pris le temps de nous informer de votre expérience. Votre feedback est précieux et nous aidera à améliorer nos produits. Veuillez accepter nos excuses les plus sincères.\\n\\nCordialement,\\n\\nL'équipe de [Votre marque]\"}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRajBYL6GIwH"
      },
      "source": [
        "### Router Chain: Multi-Prompt Router\n",
        "\n",
        "#### Contextual Inputs and Prompts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zu5h4tzGIwH"
      },
      "source": [
        "# Templates for different domains\n",
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "# Information about each prompt\n",
        "prompt_infos = [\n",
        "    {\"name\": \"physics\", \"description\": \"Good for answering questions about physics\", \"prompt_template\": physics_template},\n",
        "    {\"name\": \"math\", \"description\": \"Good for answering math questions\", \"prompt_template\": math_template},\n",
        "    {\"name\": \"History\", \"description\": \"Good for answering history questions\", \"prompt_template\": history_template},\n",
        "    {\"name\": \"computer science\", \"description\": \"Good for answering computer science questions\", \"prompt_template\": computerscience_template}\n",
        "]\n",
        "llm = ChatOpenAI(temperature=0, model=llm_model)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RWJ3dQ1GIwJ"
      },
      "source": [
        "#### Making the Prompt Destination Templates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yxbSWiSGIwJ"
      },
      "source": [
        "# Create chains for each prompt template\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ_Osz6UGIwJ"
      },
      "source": [
        "#### Multi-Prompt Router Template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV4hAjiWGIwK"
      },
      "source": [
        "# Template for multi-prompt router\n",
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAbO8ZogGIwK"
      },
      "source": [
        "#### Creating the Final Router Prompt and Chain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XpQQ2NGGIwK"
      },
      "source": [
        "# Create the router prompt template\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHwKviAiGIwK"
      },
      "source": [
        "#### Create the Multi-Prompt Chain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM6zNil9GIwK"
      },
      "source": [
        "# Create the multi-prompt chain\n",
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1AFWKI3GIwK"
      },
      "source": [
        "#### Test the Interpreter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "0wYHAQ98GIwK",
        "outputId": "dbc64507-82bb-4d08-d9d5-2935a92d3ae5"
      },
      "source": [
        "# Test for Physics Interpreter\n",
        "chain.run(\"What is black body radiation?\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Black body radiation refers to the electromagnetic radiation emitted by an object that absorbs all incident radiation and reflects or transmits none. It is called \"black body\" because it absorbs all wavelengths of light, appearing black at room temperature. \\n\\nAccording to Planck\\'s law, black body radiation is characterized by a continuous spectrum of wavelengths and intensities, which depend on the temperature of the object. As the temperature increases, the peak intensity of the radiation shifts to shorter wavelengths, resulting in a change in color from red to orange, yellow, white, and eventually blue at very high temperatures.\\n\\nBlack body radiation is a fundamental concept in physics and has significant applications in various fields, including astrophysics, thermodynamics, and quantum mechanics. It played a crucial role in the development of quantum theory and understanding the behavior of light and matter.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "M6FkvVkFGIwL",
        "outputId": "feead313-c2c2-47b3-da77-cab1134d82ca"
      },
      "source": [
        "# Test for Math Interpreter\n",
        "chain.run(\"what is square of (16)\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "math: {'input': 'what is the square of 16'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Thank you for your kind words! I'd be happy to help you with your question.\\n\\nTo find the square of 16, we simply multiply 16 by itself. So, 16 squared can be calculated as:\\n\\n16 * 16 = 256\\n\\nTherefore, the square of 16 is 256.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "PWpzGiQMGIwL",
        "outputId": "90b70dd1-88f4-4075-9977-a8e1624865d2"
      },
      "source": [
        "# Not related / Biology\n",
        "chain.run(\"Why does every cell in ourt body contain DNA?\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "None: {'input': 'Why does every cell in our body contain DNA?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Every cell in our body contains DNA because DNA is the genetic material that carries the instructions for the development, functioning, and reproduction of all living organisms. DNA contains the information necessary for the synthesis of proteins, which are essential for the structure and function of cells. It serves as a blueprint for the production of specific proteins that determine the characteristics and traits of an organism. Additionally, DNA is responsible for the transmission of genetic information from one generation to the next during reproduction. Therefore, every cell in our body contains DNA to ensure the proper functioning and continuity of life.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbWDj1K9GIwL"
      },
      "source": [
        "For more details consult [langchain Chains API](https://python.langchain.com/docs/modules/chains)\n",
        "\n",
        "\n",
        "## Agents: Toolkits and Wikipedia\n",
        "\n",
        "### Introduction to Agents\n",
        "\n",
        "Natural Language Models (NLMs) like GPT-3.5 often rely on preprocessed data up to a certain time, typically until 2022 for GPT-3.5. However, continuous knowledge creation or prompt-specific information requires specific tools to utilize this information and synthesize it. Essentially, agents gather various pieces of information and synthesize relevant content, potentially consulting web sources and other tools to respond to queries effectively.\n",
        "\n",
        "### Wikipedia Tool\n",
        "\n",
        "When a query is made to the language model endpoint and it doesn't find relevant information, the \"tools\" will search for information in other specified sources to generate context and responses to the question. To simulate this hypothetical case using information found on Wikipedia, we can utilize the `tools` components of LangChain. Specifically, to integrate Wikipedia functionality, we can consult more tools in [`Components.tools`](https://python.langchain.com/docs/integrations/tools)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bJh2AbmGIwL"
      },
      "source": [
        "# Import necessary methods and functions\n",
        "\n",
        "from langchain.agents import load_tools, initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "from langchain.python import PythonREPL\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Define the language model\n",
        "llm_model = 'gpt-3.5-turbo'\n",
        "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
        "\n",
        "# Load Wikipedia tool into a list of tools\n",
        "tools = load_tools([\"wikipedia\"], llm=llm)\n",
        "\n",
        "# Initialize the Wikipedia agent\n",
        "wikipedia = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose=True  # Show verbose output\n",
        ")"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vODtuKswGIwL"
      },
      "source": [
        "*Examples*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5beoamB-GIwL",
        "outputId": "337aa245-7793-46bf-aeeb-4cf15dfd5dce"
      },
      "source": [
        "# Query about the current president of Peru\n",
        "wikipedia(\"who is the current president of Peru?\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI can find the answer to this question using Wikipedia.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"wikipedia\",\n",
            "  \"action_input\": \"List of presidents of Peru\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: List of presidents of Peru\n",
            "Summary: This is a list of those who have served as President of the Republic of Peru (head of state and head of government of Peru) from its establishment to the present. The office was established by the Constituent Congress of Peru (1822), after the resignation of José de San Martín to his position as Protector of Peru and his subsequent departure from Peru. The first president was José de la Riva Agüero and the current president in office is Dina Boluarte, the first woman to hold the position. In the history of the position, there has been a series of political crises, caudillos, barracks revolt, civil wars, death of the incumbent, coups d'état, parliamentary attempts to remove the presidency, one autocoup, and vacancies dictated by the congress. The list is based on the work of the historian Jorge Basadre, constitutions, laws, and decrees in each case. Even though they were not presidents, the list includes the Libertadores José de San Martín and Simón Bolívar due to their historical relevance in the independence of Peru and its consolidation.\n",
            "\n",
            "\n",
            "\n",
            "Page: President of Peru\n",
            "Summary: The President of Peru (Spanish: Presidente del Perú), officially called the Constitutional President of the Republic of Peru (Spanish: presidente constitucional de la República del Perú), is the head of state and head of government of Peru. The president is the head of the executive branch and is the Supreme Head of the Armed Forces and National Police of Peru. The office of president corresponds to the highest magistracy in the country, making the president the highest-ranking public official in Peru.Due to broadly interpreted impeachment wording in the 1993 Constitution of Peru, the Congress of Peru can impeach the president without cause, effectively making the executive branch subject to the legislature.The president is elected to direct the general policy of the government, work with the Congress of the Republic and the Council of Ministers to enact reform, and be an administrator of the state, enforcing the Constitution of 1993 which establishes the presidential requirements, rights, and obligations. The executive branch is located at the Palacio de Gobierno, located in the historic center of Lima. The building has been used and occupied by the heads of state of Peru, dating back to Francisco Pizarro and the viceroys of Peru.\n",
            "The current president of Peru is Dina Boluarte, who succeeded Pedro Castillo on 7 December 2022.\n",
            "\n",
            "Page: List of viceroys of Peru\n",
            "Summary: This article lists the viceroys of Peru, who ruled the Viceroyalty of Peru from 1544 to 1824 in the name of the monarch of Spain. The territories under de jure rule by the viceroys included in the 16th and 17th century almost all of South America except eastern Brazil.\n",
            "\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mBased on the information from Wikipedia, the current president of Peru is Dina Boluarte. \n",
            "\n",
            "Final Answer: The current president of Peru is Dina Boluarte.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'who is the current president of Peru?',\n",
              " 'output': 'The current president of Peru is Dina Boluarte.'}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eInljS5VGIwL",
        "outputId": "85fa016b-d112-4e32-a774-3c1b652165a5"
      },
      "source": [
        "# Query about the death of Sebastian Piñera, former President of Chile\n",
        "wikipedia(\"Sebastian Piñera die? cause of death\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mQuestion: Did Sebastian Piñera die? What was the cause of death?\n",
            "Thought: I can use Wikipedia to find information about Sebastian Piñera's current status and any news about his death.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"wikipedia\",\n",
            "  \"action_input\": \"Sebastian Piñera\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: Sebastián Piñera\n",
            "Summary: Miguel Juan Sebastián Piñera Echenique (Spanish: [miˈɣel ˈxwan seβasˈtjam piˈɲeɾa etʃeˈnike] ; 1 December 1949 – 6 February 2024) was a Chilean businessman and politician who served as president of Chile from 2010 to 2014 and again from 2018 to 2022. The son of a Christian Democratic politician and diplomat, he studied business administration at the Pontifical Catholic University of Chile and economics at Harvard University. At the time of his death, he had an estimated net worth of US$2.7 billion, according to Forbes, making him the third richest person in Chile and the 1177th richest person in the world.A member of the liberal-conservative National Renewal party, he served as a senator for the East Santiago district from 1990 to 1998, running for the presidency in the 2005 election, which he lost to Michelle Bachelet, and again, successfully, in 2010. As a result, he became Chile's first conservative president to be democratically elected since 1958, and the first to hold the office since the departure of Augusto Pinochet in 1990.Following the social unrest that erupted in late 2019, Piñera's diminished capacity to govern according to the principle of presidentialism led to claims that Chile was in a state of de facto parliamentarism or should become parliamentary. The legacy of Piñera's two administrations include the reconstruction following the 2010 Chile earthquake, the rescue of 33 trapped miners in 2010, a rapid response to the COVID-19 pandemic, and the legalization of same-sex marriage in Chile in 2021–2022. Piñera died in a helicopter crash on Lake Ranco on 6 February 2024 at age 74.\n",
            "\n",
            "Page: 2024 in Chile\n",
            "Summary: The following is a list of events in the year 2024 in Chile.\n",
            "\n",
            "Page: 2005–06 Chilean general election\n",
            "Summary: General elections were held in Chile on Sunday, 11 December 2005 to elect the president and members of the National Congress . None of the four presidential candidates received an absolute majority, leading to a runoff election between the top two candidates — Michelle Bachelet from the Coalition of Parties for Democracy and Sebastián Piñera from National Renewal — on Sunday, 15 January 2006. Bachelet was victorious with 53.49% of the vote. She succeeded President Ricardo Lagos on 11 March 2006, for a period of four years, after Congress reformed the Constitution in September 2005 and reduced the term from six years.\n",
            "\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mBased on the information from Wikipedia, Sebastian Piñera died in a helicopter crash on Lake Ranco on February 6, 2024, at the age of 74. The cause of death was the helicopter crash. \n",
            "\n",
            "Thought: I have found the answer to the question about Sebastian Piñera's death and the cause of death.\n",
            "Final Answer: Sebastian Piñera died in a helicopter crash on February 6, 2024. The cause of death was the helicopter crash.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Sebastian Piñera die? cause of death',\n",
              " 'output': 'Sebastian Piñera died in a helicopter crash on February 6, 2024. The cause of death was the helicopter crash.'}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJeBijBJGIwL"
      },
      "source": [
        "This code sets up an agent to interact with Wikipedia for retrieving information. It initializes the agent with Wikipedia as one of the tools to consult when the language model endpoint doesn't have relevant information. Then, it demonstrates how to use the agent to query information, such as the current president of Peru and details about the death of Sebastian Piñera, the former President of Chile.\n",
        "\n",
        "## Python REPL Tool\n",
        "\n",
        "The Python REPL (Read-Eval-Print Loop) tool allows for the execution of Python code based on the questions asked, providing solutions accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF_V_DURGIwL"
      },
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_experimental.tools import PythonREPLTool\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "\n",
        "# Define instructions for the agent\n",
        "instructions = \"\"\"You are an agent designed to write and execute Python code to answer questions.\n",
        "You have access to a Python REPL, which you can use to execute Python code.\n",
        "If you get an error, debug your code and try again.\n",
        "Only use the output of your code to answer the question.\n",
        "You might know the answer without running any code, but you should still run the code to get the answer.\n",
        "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
        "\"\"\"\n",
        "# Pulling a base prompt from Hub\n",
        "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
        "# Partially filling the base prompt with instructions\n",
        "prompt = base_prompt.partial(instructions=instructions)\n",
        "\n",
        "# Initialize the Python REPL tool\n",
        "tools = [PythonREPLTool()]\n",
        "\n",
        "# Create an agent using OpenAI functions\n",
        "agent = create_openai_functions_agent(ChatOpenAI(temperature=0), tools, prompt)\n",
        "# Initialize an AgentExecutor for invoking the agent\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BobezFLoGIwM"
      },
      "source": [
        "*Example*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGa3KRyqGIwM",
        "outputId": "e225ad72-d03d-47e2-ae86-bd6a80c815ba"
      },
      "source": [
        "# Querying the agent for the 10th Fibonacci number\n",
        "agent_executor.invoke({\"input\": \"What is the 10th Fibonacci number?\"})"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Python_REPL` with `def fibonacci(n):\n",
            "    if n <= 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        return fibonacci(n-1) + fibonacci(n-2)\n",
            "\n",
            "fibonacci(10)`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThe 10th Fibonacci number is 55.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the 10th Fibonacci number?',\n",
              " 'output': 'The 10th Fibonacci number is 55.'}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqDNZVNzGIwM",
        "outputId": "7f42c4a2-1c26-4aa1-efa1-fe7b318b3010"
      },
      "source": [
        "# Sorting a list of customers by last name and then first name\n",
        "customer_list = [\n",
        "    [\"Harrison\", \"Chase\"],\n",
        "    [\"Lang\", \"Chain\"],\n",
        "    [\"Dolly\", \"Too\"],\n",
        "    [\"Elle\", \"Elem\"],\n",
        "    [\"Geoff\", \"Fusion\"],\n",
        "    [\"Trance\", \"Former\"],\n",
        "    [\"Jen\", \"Ayai\"],\n",
        "]\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": f\"\"\"\n",
        "        Sort these customers by last name and then first name and print the output: {customer_list}\n",
        "        \"\"\"\n",
        "    }\n",
        ")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Python_REPL` with `customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Python_REPL` with `sorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Python_REPL` with `sorted_customers`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThe sorted list of customers by last name and then first name is:\n",
            "\n",
            "[['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"\\n        Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\n        \",\n",
              " 'output': \"The sorted list of customers by last name and then first name is:\\n\\n[['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\"}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}