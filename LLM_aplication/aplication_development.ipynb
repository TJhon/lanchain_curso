{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install openai langchain tiktoken wikipedia langchain-experimental lanchainhub docarray -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aplication Development\n",
        "\n",
        "## Repaso rapido\n",
        "\n",
        "### Prompts\n",
        "\n",
        "\n",
        "Los \"prompts\" son instrucciones o frases cortas que se utilizan para guiar a un modelo de lenguaje como un Modelo de Lenguaje de Aprendizaje de Máquina (LLM) sobre qué tipo de texto generar. Estas instrucciones pueden ser tan simples como una palabra o frase, o pueden ser párrafos completos, dependiendo del tipo de respuesta deseada y del modelo específico que se esté utilizando.\n",
        "\n",
        "### `Langchain`\n",
        "\n",
        "`LangChain` is a *framework* for developing applications powered by language models. It enables applications that:\n",
        "\n",
        "Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)\n",
        "Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)\n",
        "\n",
        "<!-- \n",
        "### Langchain: Question and Answer with DocArrayInMemorySearch\n",
        "\n",
        "Anteriormente se uso Chroma para almacenar la base de datos, ahora se usara `DocArrayInMemorySearch` para hacer las consultas, el procedimiento es similar -->\n",
        "\n",
        "\n",
        "<!-- #### \n",
        "\n",
        "- Question and Answer\n",
        "  - leer un documento\n",
        "  - guardar el documento en una base de datos pasando previamente por un modelo de embeddings\n",
        "  - hacer las consultas\n",
        "  - la base de datos retorna una lista de elementos que segun la coincidencia de vectoores, son importantes para responder la consulta\n",
        "  - los documentos se pasan por un motor de llm para sintetizar y generar una respuesta adecuada  -->\n",
        "\n",
        "\n",
        "## Model Parsel\n",
        "\n",
        "### Chat API: Open AI\n",
        "\n",
        "- [Open ai version 1.0.0](https://github.com/openai/openai-python/discussions/742)\n",
        "\n",
        "En internet se puede encontrar varios tutoriales de como usar la API de openai, pero \n",
        "si estos tutoriales son antes de noviembre es posible que esten desactualizados, ya que si se instala `pip install openai` se tendra una version de 1.x.x, mientras que los tutoriales antes de esa fecha estaban trabajando con la version beta.\n",
        "\n",
        "Por ejemplo para poder acceder al chat de `openai`, note en el siguiente ejemplo que el nuevo modelo de creacion se basa en crear una instancia del modulo `OpenAI()` en vez de tenerlo globalmente, y para acceder a los metodos pasamos de `openai.ChatCompletition` a `OpenAI().chat.completions`\n",
        "\n",
        "```python\n",
        "# old\n",
        "import openai\n",
        "\n",
        "completion = openai.ChatCompletion.acreate(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
        "\n",
        "# new\n",
        "from openai import OpenAI()\n",
        "\n",
        "client = OpenAI()\n",
        "completion = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
        "```\n",
        "\n",
        "En cuanto a las respuestas, se basa en [`pydantic models`](https://docs.pydantic.dev/latest/concepts/models/) lo que quiere decir que ya no se trabajan con diccionarios sino mas bien con atributos, sin embargo `pydantic` permite convertir a un diccionario con `model.model_dump()`.\n",
        "\n",
        "```python\n",
        "\n",
        "# old \n",
        "import json\n",
        "import openai\n",
        "\n",
        "completion = openai.Completion.create(model='gpt-3.5-turbo')\n",
        "print(completion['choices'][0]['text']) # /// old\n",
        "print(completion.get('usage')) #  /// old\n",
        "print(json.dumps(completion, indent=2))\n",
        "\n",
        "# new\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "completion = client.completions.create(model='gpt-3.5-turbo')\n",
        "print(completion.choices[0].text)  #/// new\n",
        "print(dict(completion).get('usage'))    #/// new\n",
        "print(completion.model_dump_json(indent=2))\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "*Example*\n",
        "\n",
        "Primero debemos de llamar a la clase,  crear el cliente de OpenAI, y pasar como \n",
        "parametro nuestra api_key sin embargo si se esta usando archivos de entorno `.env` solo se debe tener correctamente el nombre de la variable en este caso `OPENAI_API_KEY`. En el cual podemos definir nuestra api_key\n",
        "\n",
        "![](https://imgs.search.brave.com/j6GBXNcpKlQxBOvfNFyojLGZjzHPUAYi71MoT5kmFes/rs:fit:860:0:0/g:ce/aHR0cHM6Ly9sYWJz/LnRoaW5rdGVjdHVy/ZS5jb20vc3RvcmFn/ZS9pbWFnZS01LnBu/Zw)\n",
        "\n",
        "Para poder acceder a la variable debemos de correr el siguiente codigo y llamar a nuestra variable de entorno"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = api_key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para poder acceder directamente a nuestra respuesta, se crea una funcion la cual toma como parametros la pregunta (o prompt) y el modelo que queremos usar, recalcando como ahora se usa respuestas de pydantic entonces debemos acceder a la respuesta con los atributos en este caso es `completion.choices[0].message.content`. Problemos con una pregunta general y un prompt.\n",
        "\n",
        "*Example*\n",
        "\n",
        "\n",
        "### Open AI and LangChain\n",
        "\n",
        "LangChain afortunadamente actualiza su codigo conforme sus dependencias tambien\n",
        "lo hacen, por lo que en las ultimas versiones de langchain ya esta implementada esta migracion.\n",
        "\n",
        "Para poder usar esta forma de llamar a openai, podemos replicar el anterior ejemplo con prompts, primero llamamos al modulo de `ChatOpenAI`, y definimos el prompt con las variables que queremos, y lo guardamos en la variable message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "chat # callable([list])\n",
        "\n",
        "template_string = \"\"\"\"\"\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "variable1\n",
        "variable2\n",
        "\n",
        "message = prompt_template.format_messages(\n",
        "    # \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por ultimo obtenemos la respuesta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "response = chat(message)\n",
        "print(response.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output Parser\n",
        "\n",
        "En algunos casos se querra que el tipo de respuesta tenga alguna estructura, por ejemplo un json, sin embargo cuando obtenemos la respuesta de langchain notamos que el resultado es un String y no podemos acceder a los elementos. \n",
        "\n",
        "\n",
        "Afortunadamente se puede pasar el output string a un dicionario de python, primero importamos `ResponseSchema` y `StructuredOutputParser`, luego creamos los elemetos que se quiere extraer con una descripcion para que el modelo sepa que colocar y parsear al typo requerido, por ultimo pasamos esto por las instrucciones de formato, dentro del prompt inicial.\n",
        "\n",
        "\n",
        "<!-- codigo -->\n",
        "\n",
        "Cuando se corra el modelo se obtiene una respuesta tal cual se puso en las instrucciones de formato pasamos por el metodo parse del contenido y este retornara el diccionario de python.\n",
        "\n",
        "<!-- Adicionalmente se puede consultar https://www.youtube.com/watch?v=I4mFqyqFkxg -->\n",
        "\n",
        "## LangChain Memory\n",
        "\n",
        "usualmente cuando interactuamos con `ChatGPT`, las respuestas que tenemos de una a la siguiente estan relacionadas ya que `ChatGOT` trabaja contextualizando (chains) y tomando como referencia los inputs y outputs anteriores para formular la ultima respuesta.\n",
        "\n",
        "![](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)\n",
        "\n",
        "Para poder simular este escenario con LangChain, usaremos 4 tipos de memoria y \n",
        "\n",
        "- ConversationBufferMemory\n",
        "- ConversationBufferWindowMemory\n",
        "- ConversationTokenBufferMemory\n",
        "- ConversationSummaryMemory\n",
        "\n",
        "Para poder usar estos metodos podemos usar esta pequena referencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.chat_models import ChatOpenAI #///llm model\n",
        "from langchain.chains import ConversationChains # chain\n",
        "\n",
        "llm_model = 'gpt-3.5-turbo'\n",
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "# ///// only reference\n",
        "from langchain.memory import Conversation{Method}Memory as Memory\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.0, model='gpt-3.5-turbo')\n",
        "memory = Memory()\n",
        "conversation = ConversationChains(\n",
        "    llm = llm,\n",
        "    memory = memory\n",
        ")\n",
        "\n",
        "conversation.predict(input='Question') # Add memory (input, output)\n",
        "memory.load_memory_variables({}) # Returns memory (history)\n",
        "memory.save_context({\"input\": \"user_content\", \"output\": \"AI response\"}) # Add manual memory\n",
        "```\n",
        "\n",
        "### ConversationBufferMemory\n",
        "\n",
        "Using this in a chain (setting verbose=True so we can see the prompt)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.memory import ConversationBufferMemory as Memory \n",
        "\n",
        "memory = Memory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "conversation.predict(input=\"Hi there!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conversation.predict(input=\"Tell me about yourself.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conversation.load_memory_variables({})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ConversationBufferWindowMemory\n",
        "\n",
        "`ConversationBufferWindowMemory` keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.memory import ConversationBufferMemory as Memory \n",
        "\n",
        "memory = Memory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "memory = ConversationBufferWindowMemory( k=1)\n",
        "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
        "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ConversationTokenBufferMemory\n",
        "\n",
        "`ConversationTokenBufferMemory` keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.memory import ConversationBufferMemory as Memory \n",
        "\n",
        "memory = Memory(max_token_limit = 60)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "conversation_with_summary.predict(input=\"Hi, what's up?\")\n",
        "conversation_with_summary.predict(input=\"Just working on writing some code!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ConversationSummaryMemory\n",
        "\n",
        "`ConversationSummaryBufferMemory` combines the two ideas. It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both. It uses token length rather than number of interactions to determine when to flush interactions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.memory import ConversationBufferMemory as Memory\n",
        "\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory = Memory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
        "memory.save_context(\n",
        "    {\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"}\n",
        ")\n",
        "\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
        "\n",
        "conversation.predict(input=\"What would be a good demo to show?\")\n",
        "memory.load_memory_variables({})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LangChain: Chains\n",
        "\n",
        "Anteriormente se uso cadenas, pero de manera rapida para poder obtener una respuesta \n",
        "del modelo llm.\n",
        "\n",
        "Chains in LangChain encompass sequences of function calls, whether to an LLM, a tool, or a data preprocessing step. LCEL (LangChain Execution Language) is the primary method for constructing these chains, offering both custom and off-the-shelf options. \n",
        "\n",
        "\n",
        "\n",
        "Combine multiple chains where the output of the one chain is the input of the next chain\n",
        "\n",
        "- LLMchain:  It's just the combination of the LLM and the prompt (simple chain)\n",
        "\n",
        "- simple sequatial chain:  un cadena tiene un output el cual es el input de la cadena 2 el cual tiene el output final\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/1*sYdb7ca9vcmDV0gOiNaruQ.png)\n",
        "\n",
        "- sequential chain: Comparing it to the above chain, you can notice that any \n",
        "step in the chain can take in multiple input variables. \n",
        "This is useful when you have more complicated downstream \n",
        "chains that need to be a composition of multiple \n",
        "previous chains. \n",
        "\n",
        "![](https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/7_Bpn6EWD-thumbnail_webp-600x300.webp)\n",
        "\n",
        "- Router chain: This chain uses an LLM to route between potential options. If you have multiple sub chains, \n",
        "each of which specialized for a particular type of input, \n",
        "you could have a router chain which first \n",
        "decides which subchain to pass it to and then passes it to \n",
        "that chain. \n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:700/1*0TDSAfaL2Q46TnFWkQTagg.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate as CPrompt\n",
        "from langchain.chains import LLMChain # simple chain\n",
        "from langchain.chains import SimpleSequentialChain # Simple Sequential Chain\n",
        "from langchain.chains import SequentialChain # Sequential Chain\n",
        "# Router Chain\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm_model = 'gpt-3.5-turbo'\n",
        "llm = ChatOpenAI(temperature=0.9, model=llm_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM chain:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prompt = CPrompt.from_template(\"{input_}\")\n",
        "chain = LLMChain(llm = llm, prompt = prompt)\n",
        "input_ = \"\"\n",
        "chain.run(input_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SimpleSequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "first_prompt = CPrompt.from_template(\"{input_1}\")\n",
        "first_chain = LLMChain(llm=llm, prompt=first_prompt)\n",
        "second_prompt = CPrompt.from_template(\"{input_2}\")\n",
        "second_chain = LLMChain(llm=llm, prompt=second_prompt)\n",
        "\n",
        "simple_chain = SimpleSequentialChain(\n",
        "    chains = [first_chain, second_chain],\n",
        "    verbose = True\n",
        ")\n",
        "\n",
        "input_1 = \"\"\n",
        "\n",
        "simple_chain.run(input_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SequentialChain\n",
        "\n",
        "chain one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
        "\n",
        "# prompt template 1: translate to english\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the following review to english:\"\n",
        "    \"\\n\\n{Review}\"\n",
        ")\n",
        "# chain 1: input= Review and output= English_Review\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
        "                     output_key=\"English_Review\"\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chain Two"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following review in 1 sentence:\"\n",
        "    \"\\n\\n{English_Review}\"\n",
        ")\n",
        "# chain 2: input= English_Review and output= summary\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
        "                     output_key=\"summary\"\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chain Three"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# prompt template 3: translate to english\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What language is the following review:\\n\\n{Review}\"\n",
        ")\n",
        "# chain 3: input= Review and output= language\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                       output_key=\"language\"\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chain Four"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# prompt template 4: follow up message\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following \"\n",
        "    \"summary in the specified language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
        ")\n",
        "# chain 4: input= summary, language and output= followup_message\n",
        "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
        "                      output_key=\"followup_message\"\n",
        "                     )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run with review input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "review = \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
        "\n",
        "# overall_chain: input= Review \n",
        "# and output= English_Review,summary, followup_message\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "overall_chain(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Router Chain\n",
        "\n",
        "Cotextual Inputs and Prompts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts, \n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity. \n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\", \n",
        "        \"description\": \"Good for answering questions about physics\", \n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\", \n",
        "        \"description\": \"Good for answering math questions\", \n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\", \n",
        "        \"description\": \"Good for answering history questions\", \n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\", \n",
        "        \"description\": \"Good for answering computer science questions\", \n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make the prompts destination templates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain  \n",
        "    \n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "\n",
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Template for multi prompt router template\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "    ```json\n",
        "    {{{{\n",
        "        \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "        \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "    }}}}\n",
        "    ```\\\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and finally create the final router prompt, router_chain "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the llm interpretter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chain = MultiPromptChain(\n",
        "    router_chain=router_chain, \n",
        "    destination_chains=destination_chains, \n",
        "    default_chain=default_chain, \n",
        "    verbose=True # False if you don't want to see the procces of the chain\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Test our interpreter*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Physic Interpreter\n",
        "chain.run(\"What is black body radiation?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Math interpreter\n",
        "chain.run(\"what is square of (16)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Call the default chain which itself is just a LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Not related / Biology\n",
        "chain.run(\"Why does every cell in ourt body contain DNA?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For more details consult [langchain Chains API](https://python.langchain.com/docs/modules/chains)\n",
        "\n",
        "## Agents\n",
        "\n",
        "Usualmente los modelos de lenguaje natural, se basan en datos preprocesados hasta un determinado tiemp, tomando en cuenga a GPT-3.5 el cual tiene informacion hasta el anio 2022. Sin embargo la creacion de conocimiento continua o el prompt necesita de herramientas especificas, para poder utilizar esta informacion y sintetisarlo podemos usar los conocimientos previos los cuales en escencia es, tener vvarias piezas de informacion y poder crear contenido que sea relevante, la diferencia es que ahora vamos a consultar a informacion de la web y otras herramientas para poder recopilar informacion y sintetizar la informacion para responder a la consulta. \n",
        "\n",
        "\n",
        "### Wikipedia\n",
        "\n",
        "En especifico, cuando se hace alguna consulta al modelo de llm, y no encuentra en el endpoint, los \"`tools`\" buscara informacion en otras fuentes de informacion que especifiquemos para poder generar el contexto y respuesta a la pregunta. Para ello vamos a usar los componentes `tools` de langchain, en especifico para simular el caso hipotetico informacion que se encuentra en wikypedia, se puede consultar mas tools en [`Components.tools`](https://python.langchain.com/docs/integrations/tools)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Methods and Fucntions\n",
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.agents import load_tools, initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "from langchain.python import PythonREPL\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "llm_model = 'gpt-3.5-turbo'\n",
        "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
        "# add tools into a list\n",
        "tools = load_tools([\"wikipedia\"], llm=llm)\n",
        "# initialize agent\n",
        "wikipedia = initialize_agent(\n",
        "    tools,  #wiki\n",
        "    llm, \n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose = True # show \n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Example*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wikipedia(\"who is the current president of Peru?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Chile: Sebastian Pinera https://en.wikipedia.org/wiki/Sebasti%C3%A1n_Pi%C3%B1era\n",
        "wikipedia(\"Sebastian Piñera die? cause of death\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Python REPL\n",
        "\n",
        "Este se usa para poder ejecutar python conforme a la pregunta que se le haga, para poder dar solucion a la respuesta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_experimental.tools import PythonREPLTool\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "# tools = [PythonREPLTool()]\n",
        "\n",
        "instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
        "You have access to a python REPL, which you can use to execute python code.\n",
        "If you get an error, debug your code and try again.\n",
        "Only use the output of your code to answer the question. \n",
        "You might know the answer without running any code, but you should still run the code to get the answer.\n",
        "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
        "\"\"\"\n",
        "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
        "prompt = base_prompt.partial(instructions=instructions)\n",
        "\n",
        "tools = [PythonREPLTool()]\n",
        "\n",
        "agent = create_openai_functions_agent(ChatOpenAI(temperature=0), tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Example*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agent_executor.invoke({\"input\": \"What is the 10th fibonacci number?\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "customer_list = [\n",
        "    [\"Harrison\", \"Chase\"],\n",
        "    [\"Lang\", \"Chain\"],\n",
        "    [\"Dolly\", \"Too\"],\n",
        "    [\"Elle\", \"Elem\"],\n",
        "    [\"Geoff\", \"Fusion\"],\n",
        "    [\"Trance\", \"Former\"],\n",
        "    [\"Jen\", \"Ayai\"],\n",
        "]\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": f\"\"\"\n",
        "        \n",
        "        \n",
        "        Sort these customers by \\\n",
        "        last name and then first name \\\n",
        "        and print the output: {customer_list}\n",
        "        \"\"\"\n",
        "    }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}