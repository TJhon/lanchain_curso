{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install openai langchain tiktoken wikipedia langchain-experimental langchainhub docarray -q "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Application Development\n",
        "\n",
        "## Quick Overview\n",
        "\n",
        "### Prompts\n",
        "\n",
        "Prompts are short instructions or phrases used to guide a language model like a Machine Learning Language Model (LLM) on what type of text to generate. These instructions can be as simple as a word or phrase, or they can be complete paragraphs, depending on the desired response type and the specific model being used.\n",
        "\n",
        "### Langchain\n",
        "\n",
        "`LangChain` is a framework for developing applications powered by language models. It enables applications that:\n",
        "\n",
        "- Are context-aware: Connect a language model to sources of context (prompt instructions, few-shot examples, content to ground its response in, etc.).\n",
        "- Reason: Rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.).\n",
        "\n",
        "\n",
        "\n",
        "<!-- \n",
        "### Langchain: Question and Answer with DocArrayInMemorySearch\n",
        "\n",
        "Anteriormente se uso Chroma para almacenar la base de datos, ahora se usara `DocArrayInMemorySearch` para hacer las consultas, el procedimiento es similar -->\n",
        "\n",
        "\n",
        "<!-- #### \n",
        "\n",
        "- Question and Answer\n",
        "  - leer un documento\n",
        "  - guardar el documento en una base de datos pasando previamente por un modelo de embeddings\n",
        "  - hacer las consultas\n",
        "  - la base de datos retorna una lista de elementos que segun la coincidencia de vectoores, son importantes para responder la consulta\n",
        "  - los documentos se pasan por un motor de llm para sintetizar y generar una respuesta adecuada  -->\n",
        "\n",
        "\n",
        "\n",
        "## Model Parsel\n",
        "\n",
        "### Chat API: Open AI\n",
        "\n",
        "- [Open ai version 1.0.0](https://github.com/openai/openai-python/discussions/742)\n",
        "\n",
        "On the internet, several tutorials can be found on how to use the OpenAI API, but if these tutorials are dated before November, they might be outdated. This is because installing `pip install openai` will provide a version of 1.x.x, whereas tutorials before that date were working with the beta version.\n",
        "\n",
        "For example, to access the OpenAI chat, note in the following example that the new model of creation is based on creating an instance of the `OpenAI()` module instead of having it globally, and to access the methods we switch from `openai.ChatCompletion` to `OpenAI().chat.completions`:\n",
        "\n",
        "\n",
        "```python\n",
        "# old\n",
        "import openai\n",
        "\n",
        "completion = openai.ChatCompletion.acreate(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
        "\n",
        "# new\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "completion = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
        "```\n",
        "\n",
        "Regarding responses, it relies on [`pydantic models`](https://docs.pydantic.dev/latest/concepts/models/), meaning that dictionaries are no longer used, but rather attributes. However, `pydantic` allows conversion to a dictionary with `model.dict()`:\n",
        "\n",
        "```python\n",
        "\n",
        "# old \n",
        "import json\n",
        "import openai\n",
        "\n",
        "completion = openai.Completion.create(model='gpt-3.5-turbo', messages=[{}])\n",
        "print(completion['choices'][0]['text']) # /// old\n",
        "print(completion.get('usage')) #  /// old\n",
        "print(json.dumps(completion, indent=2))\n",
        "\n",
        "# new\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "completion = client.completions.create(model='gpt-3.5-turbo', messages=[{}])\n",
        "print(completion.choices[0].message.content)  #/// new\n",
        "```\n",
        "\n",
        "First, we need to call the class, create the OpenAI client, and pass our api_key as a parameter. However, if we are using environment files `.env`, we only need to have the variable name correctly set, in this case `OPENAI_API_KEY`. In which we can define our api_key.\n",
        "\n",
        "![](https://imgs.search.brave.com/j6GBXNcpKlQxBOvfNFyojLGZjzHPUAYi71MoT5kmFes/rs:fit:860:0:0/g:ce/aHR0cHM6Ly9sYWJz/LnRoaW5rdGVjdHVy/ZS5jb20vc3RvcmFn/ZS9pbWFnZS01LnBu/Zw)\n",
        "\n",
        "To access the variable, we must run the following code and call our environment variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = api_key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To directly access our response, a function is created which takes the question (or prompt) and the model we want to use as parameters, emphasizing how responses from pydantic are now used. Therefore, we must access the response with the attributes, in this case `completion.choices[0].message.content`. Let's try with a general question and a prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How do I output all files in a directory using Python?\",\n",
        "        },\n",
        "    ],\n",
        ")\n",
        "print(completion.choices[0].message.content)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Open AI and LangChain\n",
        "\n",
        "Fortunately, LangChain updates its code as its dependencies do, so in the latest versions of LangChain, this migration is already implemented.\n",
        "\n",
        "To use this way of calling OpenAI, we can replicate the previous example with prompts. First, we call the `ChatOpenAI` module and define the prompt with the variables we want, and save it in the `message` variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "# chat # callable([list])\n",
        "\n",
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "variable1 = \"\"\"American English \\\n",
        "in a calm and respectful tone\n",
        "\"\"\"\n",
        "variable2 = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse, \\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\"\n",
        "\n",
        "message = prompt_template.format_messages(style=variable1, text=variable2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we get the response."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "response = chat(message)\n",
        "print(response.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output Parser\n",
        "\n",
        "In some cases, you may want the response type to have a certain structure, such as JSON. However, when we obtain the response from LangChain, we notice that the result is a string and we cannot access its elements.\n",
        "\n",
        "Fortunately, we can convert the output string to a Python dictionary. First, we import `ResponseSchema` and `StructuredOutputParser`. Then, we create the elements that we want to extract with a description so that the model knows what to parse and format as required. Finally, we pass this through the format instructions within the initial prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Base, repeat steps\n",
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\"\n",
        "\n",
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "\n",
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "response = chat(messages)\n",
        "\n",
        "print('Content', response.content)\n",
        "print(\"type\", type(response.content))\n",
        "# It's just a String"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parse Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "# Define the Schemas, with descriptions\n",
        "gift_schema = ResponseSchema(name=\"gift\",\n",
        "                             description=\"Was the item purchased\\\n",
        "                             as a gift for someone else? \\\n",
        "                             Answer True if yes,\\\n",
        "                             False if not or unknown.\")\n",
        "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
        "                                      description=\"How many days\\\n",
        "                                      did it take for the product\\\n",
        "                                      to arrive? If this \\\n",
        "                                      information is not found,\\\n",
        "                                      output -1.\")\n",
        "price_value_schema = ResponseSchema(name=\"price_value\",\n",
        "                                    description=\"Extract any\\\n",
        "                                    sentences about the value or \\\n",
        "                                    price, and output them as a \\\n",
        "                                    comma separated Python list.\")\n",
        "\n",
        "response_schemas = [gift_schema, \n",
        "                    delivery_days_schema,\n",
        "                    price_value_schema]\n",
        "\n",
        "# Define the method into a `output` parser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run LLm\n",
        "review_template_2 = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product\\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "text: {text}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
        "\n",
        "messages = prompt.format_messages(text=customer_review, \n",
        "                                format_instructions=format_instructions)\n",
        "response = chat(messages)\n",
        "print(\"content\", response.content)\n",
        "print(\"type\", type(response.content))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the model is run, the response obtained is as per the format instructions provided. We then pass it through the `parse` method of the content, which returns the Python dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get dictionary\n",
        "\n",
        "result_dict = output_parser.parse(response.content)\n",
        "result_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LangChain Memory\n",
        "\n",
        "When interacting with `ChatGPT`, the responses from one interaction to the next are often related because `ChatGPT` contextualizes its responses, referencing previous inputs and outputs to formulate the latest response.\n",
        "\n",
        "![](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)\n",
        "\n",
        "To simulate this scenario with LangChain, we utilize four types of memory:\n",
        "\n",
        "- ConversationBufferMemory\n",
        "- ConversationBufferWindowMemory\n",
        "- ConversationTokenBufferMemory\n",
        "- ConversationSummaryMemory\n",
        "<!-- - **ConversationBufferMemory**: Records and manages previous parts of a conversation, allowing language models to maintain conversational context. It's crucial for applications like chatbots to ensure a seamless conversational flow.\n",
        "\n",
        "- **ConversationBufferWindowMemory**: Retains a window of recent conversation exchanges, limiting memory growth. Useful for keeping track of recent terms without accumulating excessive memory.\n",
        "\n",
        "- **ConversationTokenBufferMemory**: Manages conversation tokens, controlling memory based on token limits. Helps manage costs associated with token-based pricing for language model calls.\n",
        "\n",
        "- **ConversationSummaryMemory**: Summarizes conversation history using an LLM, providing a concise overview of past exchanges. Useful for condensing lengthy conversations into manageable summaries, aiding memory storage efficiency. -->\n",
        "\n",
        "To use these methods, we can refer to the following example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChains\n",
        "\n",
        "llm_model = 'gpt-3.5-turbo'\n",
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "# Reference only\n",
        "from langchain.memory import Conversation{Method}Memory as Memory\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.0, model='gpt-3.5-turbo')\n",
        "memory = Memory()\n",
        "conversation = ConversationChains(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "conversation.predict(input='Question')  # Add memory (input, output)\n",
        "memory.load_memory_variables({})  # Returns memory (history)\n",
        "memory.save_context({\"input\": \"user_content\", \"output\": \"AI response\"})  # Add manual memory\n",
        "```\n",
        "\n",
        "\n",
        "### ConversationBufferMemory\n",
        "\n",
        "Using this in a chain (setting verbose=True so we can see the prompt)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.memory import ConversationBufferMemory as Memory \n",
        "\n",
        "memory = Memory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "conversation.predict(input=\"Hi there!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conversation.predict(input=\"Tell me about yourself.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conversation.load_memory_variables({})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ConversationBufferWindowMemory\n",
        "\n",
        "`ConversationBufferWindowMemory` keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory as Memory \n",
        "\n",
        "memory = Memory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
        "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ConversationTokenBufferMemory\n",
        "\n",
        "`ConversationTokenBufferMemory` keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory as Memory \n",
        "\n",
        "memory = Memory(max_token_limit=60)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory=memory,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "conversation_with_summary.predict(input=\"Hi, what's up?\")\n",
        "conversation_with_summary.predict(input=\"Just working on writing some code!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ConversationSummaryMemory\n",
        "\n",
        "`ConversationSummaryBufferMemory` combines the two ideas. It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both. It uses token length rather than number of interactions to determine when to flush interactions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.memory import ConversationSummaryMemory as Memory\n",
        "\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory = Memory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
        "memory.save_context(\n",
        "    {\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"}\n",
        ")\n",
        "\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
        "\n",
        "conversation.predict(input=\"What would be a good demo to show?\")\n",
        "memory.load_memory_variables({})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LangChain: Chains\n",
        "\n",
        "Previously, we used chains, but in a quick manner to obtain a response from the LLM model.\n",
        "\n",
        "Chains in LangChain encompass sequences of function calls, whether to an LLM, a tool, or a data preprocessing step. LCEL (LangChain Execution Language) is the primary method for constructing these chains, offering both custom and off-the-shelf options.\n",
        "\n",
        "Combine multiple chains where the output of one chain is the input of the next chain.\n",
        "\n",
        "- **LLMchain**: It's just the combination of the LLM and the prompt (simple chain).\n",
        "\n",
        "\n",
        "- **Simple Sequential Chain**: A chain where one chain's output is another chain's input, resulting in a final output.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*sYdb7ca9vcmDV0gOiNaruQ.png\" width=\"600px\" height=\"300px\">\n",
        "\n",
        "- **Sequential Chain**: Similar to the simple sequential chain but allows multiple input variables for each step, useful for more complex downstream chains.\n",
        "\n",
        "![](https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/7_Bpn6EWD-thumbnail_webp-600x300.webp)\n",
        "\n",
        "- **Router Chain**: Utilizes an LLM to route between potential options, deciding which sub-chain to pass input to based on input characteristics.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:700/1*0TDSAfaL2Q46TnFWkQTagg.jpeg\" width=\"600px\" height=\"300px\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate as CPrompt\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
        "from langchain.chains.router import MultiPromptChain, LLMRouterChain, RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm_model = 'gpt-3.5-turbo'\n",
        "llm = ChatOpenAI(temperature=0.9, model=llm_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM chain:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prompt = CPrompt.from_template(\"What is the best name to describe \\\n",
        "    a company that makes {product}?\")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "input_ = \"Magic wands in the Harry Potter universe\"\n",
        "chain.run(input_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SimpleSequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "first_prompt = CPrompt.from_template(\n",
        "    \"What is the best name to describe \\\n",
        "    a company that makes {product}?\"\n",
        ")\n",
        "first_chain = LLMChain(llm=llm, prompt=first_prompt)\n",
        "second_prompt = CPrompt.from_template(\n",
        "    \"Write a 20 words description for the following \\\n",
        "    company:{company_name}\"\n",
        ")\n",
        "second_chain = LLMChain(llm=llm, prompt=second_prompt)\n",
        "\n",
        "simple_chain = SimpleSequentialChain(\n",
        "    chains=[first_chain, second_chain],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "input_1 = \"Magic wands in the Harry Potter universe\"\n",
        "\n",
        "simple_chain.run(input_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.chains import SequentialChain\n",
        "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
        "\n",
        "# prompt template 1: translate to english\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the following review to english:\"\n",
        "    \"\\n\\n{Review}\"\n",
        ")\n",
        "# chain 1: input= Review and output= English_Review\n",
        "chain_one = LLMChain(\n",
        "    llm=llm, prompt=first_prompt, \n",
        "    output_key=\"English_Review\"\n",
        "    )\n",
        "\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following review in 1 sentence:\"\n",
        "    \"\\n\\n{English_Review}\"\n",
        ")\n",
        "# chain 2: input= English_Review and output= summary\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
        "                     output_key=\"summary\"\n",
        "                    )\n",
        "# prompt template 3: translate to english\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What language is the following review:\\n\\n{Review}\"\n",
        ")\n",
        "# chain 3: input= Review and output= language\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                       output_key=\"language\"\n",
        "                      )\n",
        "\n",
        "# prompt template 4: follow up message\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following \"\n",
        "    \"summary in the specified language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
        ")\n",
        "# chain 4: input= summary, language and output= followup_message\n",
        "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
        "                      output_key=\"followup_message\"\n",
        "                     )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "review = \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
        "\n",
        "# overall_chain: input= Review\n",
        "# and output= English_Review,summary, followup_message\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "overall_chain(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Router Chain: Multi-Prompt Router\n",
        "\n",
        "#### Contextual Inputs and Prompts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Templates for different domains\n",
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts, \n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity. \n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "# Information about each prompt\n",
        "prompt_infos = [\n",
        "    {\"name\": \"physics\", \"description\": \"Good for answering questions about physics\", \"prompt_template\": physics_template},\n",
        "    {\"name\": \"math\", \"description\": \"Good for answering math questions\", \"prompt_template\": math_template},\n",
        "    {\"name\": \"History\", \"description\": \"Good for answering history questions\", \"prompt_template\": history_template},\n",
        "    {\"name\": \"computer science\", \"description\": \"Good for answering computer science questions\", \"prompt_template\": computerscience_template}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Making the Prompt Destination Templates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create chains for each prompt template\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain  \n",
        "    \n",
        "# Create descriptions for prompt destinations\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "\n",
        "# Default prompt and chain\n",
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Multi-Prompt Router Template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Template for multi-prompt router\n",
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "    ```json\n",
        "    {{\n",
        "        \"destination\": string \\ name of the prompt to use or \"DEFAULT\",\n",
        "        \"next_input\": string \\ a potentially modified version of the original input\n",
        "    }}\n",
        "    \\```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_input\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating the Final Router Prompt and Chain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create the router prompt template\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "# Create the router chain\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create the Multi-Prompt Chain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create the multi-prompt chain\n",
        "chain = MultiPromptChain(\n",
        "    router_chain=router_chain, \n",
        "    destination_chains=destination_chains, \n",
        "    default_chain=default_chain, \n",
        "    verbose=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test the Interpreter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test for Physics Interpreter\n",
        "chain.run(\"What is black body radiation?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test for Math Interpreter\n",
        "chain.run(\"what is square of (16)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Not related / Biology\n",
        "chain.run(\"Why does every cell in ourt body contain DNA?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For more details consult [langchain Chains API](https://python.langchain.com/docs/modules/chains)\n",
        "\n",
        "\n",
        "## Agents: Toolkits and Wikipedia\n",
        "\n",
        "### Introduction to Agents\n",
        "\n",
        "Natural Language Models (NLMs) like GPT-3.5 often rely on preprocessed data up to a certain time, typically until 2022 for GPT-3.5. However, continuous knowledge creation or prompt-specific information requires specific tools to utilize this information and synthesize it. Essentially, agents gather various pieces of information and synthesize relevant content, potentially consulting web sources and other tools to respond to queries effectively.\n",
        "\n",
        "### Wikipedia Tool\n",
        "\n",
        "When a query is made to the language model endpoint and it doesn't find relevant information, the \"tools\" will search for information in other specified sources to generate context and responses to the question. To simulate this hypothetical case using information found on Wikipedia, we can utilize the `tools` components of LangChain. Specifically, to integrate Wikipedia functionality, we can consult more tools in [`Components.tools`](https://python.langchain.com/docs/integrations/tools)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import necessary methods and functions\n",
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.agents import load_tools, initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "from langchain.python import PythonREPL\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Define the language model\n",
        "llm_model = 'gpt-3.5-turbo'\n",
        "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
        "\n",
        "# Load Wikipedia tool into a list of tools\n",
        "tools = load_tools([\"wikipedia\"], llm=llm)\n",
        "\n",
        "# Initialize the Wikipedia agent\n",
        "wikipedia = initialize_agent(\n",
        "    tools,\n",
        "    llm, \n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose=True  # Show verbose output\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Examples*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Query about the current president of Peru\n",
        "wikipedia(\"who is the current president of Peru?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Query about the death of Sebastian Piñera, former President of Chile\n",
        "wikipedia(\"Sebastian Piñera die? cause of death\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code sets up an agent to interact with Wikipedia for retrieving information. It initializes the agent with Wikipedia as one of the tools to consult when the language model endpoint doesn't have relevant information. Then, it demonstrates how to use the agent to query information, such as the current president of Peru and details about the death of Sebastian Piñera, the former President of Chile.\n",
        "\n",
        "## Python REPL Tool\n",
        "\n",
        "The Python REPL (Read-Eval-Print Loop) tool allows for the execution of Python code based on the questions asked, providing solutions accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_experimental.tools import PythonREPLTool\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "\n",
        "# Define instructions for the agent\n",
        "instructions = \"\"\"You are an agent designed to write and execute Python code to answer questions.\n",
        "You have access to a Python REPL, which you can use to execute Python code.\n",
        "If you get an error, debug your code and try again.\n",
        "Only use the output of your code to answer the question. \n",
        "You might know the answer without running any code, but you should still run the code to get the answer.\n",
        "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
        "\"\"\"\n",
        "# Pulling a base prompt from Hub\n",
        "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
        "# Partially filling the base prompt with instructions\n",
        "prompt = base_prompt.partial(instructions=instructions)\n",
        "\n",
        "# Initialize the Python REPL tool\n",
        "tools = [PythonREPLTool()]\n",
        "\n",
        "# Create an agent using OpenAI functions\n",
        "agent = create_openai_functions_agent(ChatOpenAI(temperature=0), tools, prompt)\n",
        "# Initialize an AgentExecutor for invoking the agent\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Example*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Querying the agent for the 10th Fibonacci number\n",
        "agent_executor.invoke({\"input\": \"What is the 10th Fibonacci number?\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sorting a list of customers by last name and then first name\n",
        "customer_list = [\n",
        "    [\"Harrison\", \"Chase\"],\n",
        "    [\"Lang\", \"Chain\"],\n",
        "    [\"Dolly\", \"Too\"],\n",
        "    [\"Elle\", \"Elem\"],\n",
        "    [\"Geoff\", \"Fusion\"],\n",
        "    [\"Trance\", \"Former\"],\n",
        "    [\"Jen\", \"Ayai\"],\n",
        "]\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": f\"\"\"\n",
        "        Sort these customers by last name and then first name and print the output: {customer_list}\n",
        "        \"\"\"\n",
        "    }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}