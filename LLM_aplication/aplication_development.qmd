```{python}
%pip install openai langchain tiktoken -q
```

# Aplication Development

## Preview

- prompts: Que son los prompt y como se usan en langchain
- langchain

## Environment VAriables

Anteriormente se tenia 

## Model Parsel

### Chat API: Open AI

- [Open ai version 1.0.0](https://github.com/openai/openai-python/discussions/742)

En internet se puede encontrar varios tutoriales de como usar la API de openai, pero 
si estos tutoriales son antes de noviembre es posible que esten desactualizados, ya que si se instala `pip install openai` se tendra una version de 1.x.x, mientras que los tutoriales antes de esa fecha estaban trabajando con la version beta.

Por ejemplo para poder acceder al chat de `openai`, note en el siguiente ejemplo que el nuevo modelo de creacion se basa en crear una instancia del modulo `OpenAI()` en vez de tenerlo globalmente, y para acceder a los metodos pasamos de `openai.ChatCompletition` a `OpenAI().chat.completions`

```python
# old
import openai

completion = openai.ChatCompletion.acreate(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])

# new
from openai import OpenAI()

client = OpenAI()
completion = client.chat.completions.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])
```

En cuanto a las respuestas, se basa en [`pydantic models`](https://docs.pydantic.dev/latest/concepts/models/) lo que quiere decir que ya no se trabajan con diccionarios sino mas bien con atributos, sin embargo `pydantic` permite convertir a un diccionario con `model.model_dump()`.

```python

# old 
import json
import openai

completion = openai.Completion.create(model='gpt-3.5-turbo')
print(completion['choices'][0]['text']) # /// old
print(completion.get('usage')) #  /// old
print(json.dumps(completion, indent=2))

# new
from openai import OpenAI

client = OpenAI()

completion = client.completions.create(model='gpt-3.5-turbo')
print(completion.choices[0].text)  #/// new
print(dict(completion).get('usage'))    #/// new
print(completion.model_dump_json(indent=2))

```


*Example*

Primero debemos de llamar a la clase,  crear el cliente de OpenAI, y pasar como 
parametro nuestra api_key sin embargo si se esta usando archivos de entorno `.env` solo se debe tener correctamente el nombre de la variable en este caso `OPENAI_API_KEY`. En el cual podemos definir nuestra api_key

![](https://imgs.search.brave.com/j6GBXNcpKlQxBOvfNFyojLGZjzHPUAYi71MoT5kmFes/rs:fit:860:0:0/g:ce/aHR0cHM6Ly9sYWJz/LnRoaW5rdGVjdHVy/ZS5jb20vc3RvcmFn/ZS9pbWFnZS01LnBu/Zw)

Para poder acceder a la variable debemos de correr el siguiente codigo y llamar a nuestra variable de entorno

```{python}
from google.colab import userdata
import os
api_key = userdata.get('OPENAI_API_KEY')
os.environ['OPENAI_API_KEY'] = api_key
```

Para poder acceder directamente a nuestra respuesta, se crea una funcion la cual toma como parametros la pregunta (o prompt) y el modelo que queremos usar, recalcando como ahora se usa respuestas de pydantic entonces debemos acceder a la respuesta con los atributos en este caso es `completion.choices[0].message.content`. Problemos con una pregunta general y un prompt.

*Example*


### Open AI and LangChain

LangChain afortunadamente actualiza su codigo conforme sus dependencias tambien
lo hacen, por lo que en las ultimas versiones de langchain ya esta implementada esta migracion.

Para poder usar esta forma de llamar a openai, podemos replicar el anterior ejemplo con prompts, primero llamamos al modulo de `ChatOpenAI`, y definimos el prompt con las variables que queremos, y lo guardamos en la variable message.

```{python}
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

chat = ChatOpenAI(temperature=0.0, model=llm_model)
chat # callable([list])

template_string = """"""
prompt_template = ChatPromptTemplate.from_template(template_string)
variable1
variable2

message = prompt_template.format_messages(
    # 
)
```

Por ultimo obtenemos la respuesta.

```{python}
response = chat(message)
print(response.content)
```

## Output Parser

En algunos casos se querra que el tipo de respuesta tenga alguna estructura, por ejemplo un json, sin embargo cuando obtenemos la respuesta de langchain notamos que el resultado es un String y no podemos acceder a los elementos. 


Afortunadamente se puede pasar el output string a un dicionario de python, primero importamos `ResponseSchema` y `StructuredOutputParser`, luego creamos los elemetos que se quiere extraer con una descripcion para que el modelo sepa que colocar y parsear al typo requerido, por ultimo pasamos esto por las instrucciones de formato, dentro del prompt inicial.


<!-- codigo -->

Cuando se corra el modelo se obtiene una respuesta tal cual se puso en las instrucciones de formato pasamos por el metodo parse del contenido y este retornara el diccionario de python.

<!-- Adicionalmente se puede consultar https://www.youtube.com/watch?v=I4mFqyqFkxg -->

## LangChain Memory

usualmente cuando interactuamos con `ChatGPT`, las respuestas que tenemos de una a la siguiente estan relacionadas ya que `ChatGOT` trabaja contextualizando (chains) y tomando como referencia los inputs y outputs anteriores para formular la ultima respuesta.

![](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)

Para poder simular este escenario con LangChain, usaremos 4 tipos de memoria y 

- ConversationBufferMemory
- ConversationBufferWindowMemory
- ConversationTokenBufferMemory
- ConversationSummaryMemory

Para poder usar estos metodos podemos usar esta pequena referencia

```{python}
from langchain.chat_models import ChatOpenAI #///llm model
from langchain.chains import ConversationChains # chain

llm_model = 'gpt-3.5-turbo'
llm = ChatOpenAI(temperature=0.0, model=llm_model)
```

```python
# ///// only reference
from langchain.memory import Conversation{Method}Memory as Memory

llm = ChatOpenAI(temperature=0.0, model='gpt-3.5-turbo')
memory = Memory()
conversation = ConversationChains(
    llm = llm,
    memory = memory
)

conversation.predict(input='Question') # Add memory (input, output)
memory.load_memory_variables({}) # Returns memory (history)
memory.save_context({"input": "user_content", "output": "AI response"}) # Add manual memory
```

### ConversationBufferMemory

Using this in a chain (setting verbose=True so we can see the prompt).

```{python}
from langchain.memory import ConversationBufferMemory as Memory 

memory = Memory()
conversation = ConversationChain(
    llm=llm, 
    memory = memory,
    verbose=True
)

conversation.predict(input="Hi there!")
```

```{python}
conversation.predict(input="Tell me about yourself.")
```

```{python}
conversation.load_memory_variables({})
```

### ConversationBufferWindowMemory

`ConversationBufferWindowMemory` keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large.

```{python}
from langchain.memory import ConversationBufferMemory as Memory 

memory = Memory()
conversation = ConversationChain(
    llm=llm, 
    memory = memory,
    verbose=True
)

memory = ConversationBufferWindowMemory( k=1)
memory.save_context({"input": "hi"}, {"output": "whats up"})
memory.save_context({"input": "not much you"}, {"output": "not much"})

memory.load_memory_variables({})

```

### ConversationTokenBufferMemory

`ConversationTokenBufferMemory` keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions.

```{python}
from langchain.memory import ConversationBufferMemory as Memory 

memory = Memory(max_token_limit = 60)

conversation = ConversationChain(
    llm=llm, 
    memory = memory,
    verbose=False
)

conversation_with_summary.predict(input="Hi, what's up?")
conversation_with_summary.predict(input="Just working on writing some code!")
```

### ConversationSummaryMemory

`ConversationSummaryBufferMemory` combines the two ideas. It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both. It uses token length rather than number of interactions to determine when to flush interactions.

```{python}
from langchain.memory import ConversationBufferMemory as Memory

schedule = "There is a meeting at 8am with your product team. \
You will need your powerpoint presentation prepared. \
9am-12pm have time to work on your LangChain \
project which will go quickly because Langchain is such a powerful tool. \
At Noon, lunch at the italian resturant with a customer who is driving \
from over an hour away to meet you to understand the latest in AI. \
Be sure to bring your laptop to show the latest LLM demo."

memory = Memory(llm=llm, max_token_limit=100)
memory.save_context({"input": "Hello"}, {"output": "What's up"})
memory.save_context({"input": "Not much, just hanging"}, {"output": "Cool"})
memory.save_context(
    {"input": "What is on the schedule today?"}, {"output": f"{schedule}"}
)

conversation = ConversationChain(llm=llm, memory=memory, verbose=False)

conversation.predict(input="What would be a good demo to show?")
memory.load_memory_variables({})
```

## Chains

