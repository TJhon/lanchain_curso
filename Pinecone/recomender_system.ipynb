{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install  pinecone-client openai numpy pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recommendation System\n",
        "\n",
        "## Pinecone\n",
        "\n",
        "Pinecone simplifies the provision of long-term memory for high-performance AI applications. It is a managed, cloud-native vector database with a straightforward API and no infrastructure complexities. Pinecone delivers fresh, filtered query results with low latency, capable of scaling to billions of vectors.\n",
        "\n",
        "## Preview\n",
        "\n",
        "To access the resource, you need to create an account on [Pinecone](https://app.pinecone.io/). Currently, only the free version will be used, which allows for only one index storage, sufficient for the exercise.\n",
        "\n",
        "Previously, two elements of Pinecone were needed to make queries. However, now only the Pinecone API key is required to handle vectors.\n",
        "\n",
        "### Creating API KEY\n",
        "\n",
        "Login Pinecone\n",
        "\n",
        "![](figs/login-pinecone.png)\n",
        "\n",
        "Create Api Key\n",
        "\n",
        "![](figs/api-key.png)\n",
        "\n",
        "![](figs/create_api-key.png)\n",
        "\n",
        "Save Api Key\n",
        "\n",
        "![](figs/save-api-key.png)\n",
        "\n",
        "## How to Use\n",
        "\n",
        "To create the index in Pinecone, we need to import the necessary methods. Additionally, we must create the OpenAI client to generate embeddings (remember that embeddings are the vector representation of content).\n",
        "\n",
        "In this specific case, we are deleting all existing indexes with `pc.delete_index` in Pinecone because we can only have one in the free version. Additionally, we define the index name as `nameindex` and create the index (index creation typically takes between 1 to 3 minutes). Finally, we define our `index` to make queries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# pinecone\n",
        "from pinecone import Pinecone, PodSpec\n",
        "from google.colab import userdata\n",
        "\n",
        "pc_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "pc = Pinecone(api_key=pc_api_key)\n",
        "\n",
        "nameindex = \"recommended\"\n",
        "for index in pc.list_indexes():\n",
        "  pc.delete_index(index.get('name'))\n",
        "\n",
        "pc.create_index(\n",
        "    name=nameindex,\n",
        "    dimension=1536,\n",
        "    metric=\"cosine\",\n",
        "    spec=PodSpec(\n",
        "        environment='us-west1-gcp',\n",
        "        pod_type='p1.x1'\n",
        "    )\n",
        ")\n",
        "# define the endpoint\n",
        "index = pc.Index(nameindex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To better understand what embeddings are, let's create them with OpenAI. First, we need to call the embeddings method and create the embeddings by providing the text and the model to use. OpenAI works with embeddings of 1536 elements, so each embedding must be a list of that same size. Within these embeddings, there is the element `data[0].embedding`, which is the value needed to upload to Pinecone as `value`.\n",
        "\n",
        "Para entender mejor que son los embedings, haremos la creacion de este con openai, primero tenemos que llamar al metodo de embeddings y crear el embeddings tomando el texto y el modelo a utilizar. Openai trabaja con un los embeddings de 1536 elementos, por lo que cada emebddings debe ser una lista de ese mismo tamanio. Dentro de este embeddings existe el elemmeto `data[0].embeddig` el cual es el valor que se necesita para poder subir a pinecone como `value`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_openai_e = \"text-embedding-ada-002\"\n",
        "embed = openai_client.embeddings.create(input='text', model=model_openai_e)\n",
        "dir(embed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "value = embed.data[0].embedding\n",
        "print(f\"\"\"value: {value[:10]}\\n length: {len(value)}\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since this procedure will be repetitive, it will be included in a function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_embeddings(input, model=model_openai_e):\n",
        "    embed = openai_client.embeddings.create(input='text', model=model_openai_e)\n",
        "    value = embed.data[0].embedding\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to upload this vector to Pinecone, two additional elements are needed: the ID, which will serve as an identifier, and the metadata, which in Python is a dictionary and will be used for more precise queries.\n",
        "\n",
        "As an example, let's use a sample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "value_vector = np.random.rand(1536)  # length of vector\n",
        "metadata = {\n",
        "    \"test\": \"yes\",\n",
        "    \"title\": \"none\"\n",
        "}\n",
        "\n",
        "upsert_response = index.upsert(\n",
        "    vectors=[\n",
        "        (\"id_1\", value_vector, metadata)\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](figs/index-created.png)\n",
        "![](figs/data-inside-index.png)\n",
        "\n",
        "In the Pinecone console, you can see that there is now an element with the values defined previously.\n",
        "\n",
        "## Example\n",
        "\n",
        "Let's use news data, which is available at the following [link](https://www.dropbox.com/scl/fi/wruzj2bwyg743d0jzd7ku/all-the-news-3.zip?rlkey=rgwtwpeznbdadpv3f01sznwxa&dl=1%22). To use it, we can directly execute this code, which will download a zip file and extract its contents into the current folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!wget -q --show-progress -O all-the-news-3.zip \"https://www.dropbox.com/scl/fi/wruzj2bwyg743d0jzd7ku/all-the-news-3.zip?rlkey=rgwtwpeznbdadpv3f01sznwxa&dl=1\"\n",
        "\n",
        "!unzip all-the-news-3.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data is contained within a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./all-the-news-3.csv', nrows=99)\n",
        "print(df.columns, df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, only a sample will be uploaded to Pinecone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "df = pd.read_csv('./all-the-news-3.csv', nrows=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember that we need 3 elements - ID, vector, and metadata - to upload to Pinecone. We'll generate a function that generates an ID, the embedding of the news titles, and the metadata, all within a tuple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import uuid\n",
        "\n",
        "def to_update(title: str):\n",
        "    _id = str(uuid.uuid1())\n",
        "    embed = get_embeddings(title)\n",
        "    metadata = {\n",
        "        \"title\": title\n",
        "    }\n",
        "    return (_id, embed, metadata)\n",
        "\n",
        "to_update(\"We should take concerns about the health ...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "titles = df['titles'].values\n",
        "for title in tqdm(titles):\n",
        "    value = to_update(title)\n",
        "    index.upsert(value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To view recommendations from the database, we can make the request with the following prompt, where we specify in `vector` the query's embedding, `top_k` the number of items to display, and `include_metadata` to extract metadata that matches the vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "query_vector = get_embeddings(\"health\")\n",
        "response = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=10,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "def get_recommendations(index_pc, query, top_k=10):\n",
        "    query_vector = get_embeddings(query)\n",
        "    response = index_pc.query(\n",
        "        vector=query_vector,\n",
        "        top_k=top_k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "    return response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Subsequently, we can see the score of how matching our vector recommendations are with the query embed, and we can also access the metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Large Texts\n",
        "\n",
        "The previous example used the title as the embedding and metadata. Now, the exercise will involve using the content of the news as the embedding and the title as metadata. To achieve this, we need to consider that the texts can be very long, so we'll need to split them.\n",
        "\n",
        "First, let's delete the existing index to upload the content again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "name_index = 'article'\n",
        "for index in pc.list_indexes():\n",
        "  pc.delete_index(index.get('name'))\n",
        "\n",
        "pc.create_index(\n",
        "    name=name_index, \n",
        "    dimension=1536, \n",
        "    metric=\"cosine\", \n",
        "    spec=PodSpec(\n",
        "        environment='us-west1-gcp', \n",
        "        pod_type='p1.x1'\n",
        "    )\n",
        ")\n",
        "index = pc.Index(name_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll reuse the data, but modify the `get_embeddings` function because the previous one only accepted a string. However, when the text passes through `text_splitter`, it will return a list of strings, each of which must be uploaded individually to Pinecone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400, chunk_overlap=20 \n",
        ")\n",
        "\n",
        "articles = df['article'].values\n",
        "titles = df['titles'].values\n",
        "\n",
        "def get_embeddings(articles, model=\"text-embedding-ada-002\") -> list:\n",
        "   return openai_client.embeddings.create(input=articles, model=model).data\n",
        "\n",
        "def embed(embed_index, embeddings, title, prepped):\n",
        "    for embedding in embeddings:\n",
        "        _id = str(uuid.uuid1())\n",
        "        values = embedding.embedding\n",
        "        metadata = {'title': title}\n",
        "        embed_index.upsert((_id, values, metadata))\n",
        "\n",
        "for i, article in tqdm(enumerate(articles)):\n",
        "    if article is None:\n",
        "        continue\n",
        "    texts = text_splitter.split_text(article)\n",
        "    embeddings_texts = get_embeddings(texts)\n",
        "    embed(index, embeddings_texts, titles[i], prepped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our index, we need to make queries. We'll reuse the `get_recommendations` function to find the vectors that match the query from highest to lowest.\n",
        "\n",
        "Unlike before, it will search within the articles for the most similar vectors. So, if an article is highly related to the query, the response will contain many matches corresponding to the same article. However, this is where we can use the metadata, as we can classify when each different article is mentioned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "recommendation = get_recommendations(index, \"Health\")\n",
        "\n",
        "seen = {}\n",
        "for r in recommendation.matches:\n",
        "    title = r.metadata['title']\n",
        "    if title not in seen:\n",
        "        print(f\"Score: {r.score} \\t Title: {title}\")\n",
        "        seen[title] = \".\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}