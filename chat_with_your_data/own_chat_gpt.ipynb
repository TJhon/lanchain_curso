{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chat With Your Data\n",
        "\n",
        "In this course, we will endeavor to replicate [chatpdf](https://www.chatpdf.com/) and the essence of all natural language models that operate by responding within a specific context (a document).\n",
        "\n",
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install langchain openai pypdf python-dotenv chromadb lark -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structure\n",
        "\n",
        "- API keys and Environment Variables\n",
        "- Document Loading\n",
        "- Document Splitting\n",
        "- Vectorstores and Embedding (Storage)\n",
        "- Retrieval\n",
        "- Question Answering\n",
        "- Chat\n",
        "- Conclusion\n",
        "\n",
        "<!-- ![](figs/0_preview.png) -->\n",
        "![](https://python.langchain.com/assets/images/data_connection-95ff2033a8faa5f3ba41376c0f6dd32a.jpg)\n",
        "\n",
        "In this tutorial, we will delve into the essential steps for creating a natural language model. We'll begin by understanding the significance of API keys and utilizing environment variables to ensure security and privacy in our applications. Next, we'll dive into document loading, mastering the handling of various file types and data sources. Following that, we'll tackle document splitting for efficient processing. We'll then proceed to create vector stores and embeddings, crucial for representing the semantic meaning of words. Afterward, we'll explore information retrieval techniques and question-answering capabilities, culminating in the implementation of a chat system based on our model. \n",
        "\n",
        "Finally, we'll draw conclusions on the challenges and possibilities within this fascinating field of natural language processing.\n",
        "\n",
        "## API Key and Environment Variables\n",
        "\n",
        "Creating the API key from OPENAI, [here](https://platform.openai.com/account/api-keys).\n",
        "\n",
        "Once you have obtained the API key, it needs to be securely stored.\n",
        "\n",
        "There are various methods for loading this API key. One approach is to utilize environment files, which should ideally be private and included in the `.gitignore` if working in a collaborative environment. Github, for instance, detects if any keys are uploaded to the platform, triggering an alert and disabling the API key, necessitating the creation of a new one. The key can be manually entered as well.\n",
        "\n",
        "### `Dotenv`\n",
        "\n",
        "For this method, you can create a `.env` file in the working environment and list the variables as follows: `variable = \"value_variable\"`.\n",
        "\n",
        "```plaintext\n",
        "NAME_OF_VARIABLE=\"sk-xxxxxxxxxxxxxxxx\"\n",
        "```\n",
        "\n",
        "To use it, `python-dotenv` is employed, which, through the functions `load_dotenv` and `find_dotenv`, loads the variables from the `.env` file.\n",
        "\n",
        "```python\n",
        "# `!pip install python-dotenv`\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "secret_variable = os.environ['NAME_OF_VARIABLE']\n",
        "```\n",
        "\n",
        "### Colab\n",
        "\n",
        "For Colab, a form with `getpass` can be introduced, which conceals the API key when entered. However, the drawback compared to the previous method is that the API key needs to be pasted each time the file is executed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# !pip install openai\n",
        "import getpass, openai, os\n",
        "api_key = getpass.getpass(prompt=\"OPENAI - KEY: \")\n",
        "openai.apikey = api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document Loading\n",
        "\n",
        "When it comes to loading documents, two scenarios must be considered: whether the information will be extracted from the web or if it's within our working environment. In the former case, it's possible (although `langchain` already has these cases implemented) that we'll need the `requests` library to download the file or its content. For the latter case, only the relative or absolute path of the file is sufficient.\n",
        "\n",
        "All documents follow this structure, returning a list of `Document` objects containing two sub-objects: `page_content`, which is the text within, and `metadata`.\n",
        "\n",
        "```python\n",
        "from langchain.document_loaders import `Method`\n",
        "file = Method(file_path)\n",
        "file_read = file.lod()\n",
        "print(file_read[0])\n",
        "Document(\n",
        "    page_content: \"text\",\n",
        "    metadata: {\"source\": file_path, ...}\n",
        ")\n",
        "```\n",
        "\n",
        "### PDFs\n",
        "\n",
        "For PDF files, it's already implemented to read the document by URL and local path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"https://arxiv.org/pdf/2103.15348.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "print(pages[0].page_content[:100])\n",
        "print(pages[0].metadata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Web Plain Text\n",
        "\n",
        "For plain text from a URL, the `WebBaseLoader` can be utilized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://raw.githubusercontent.com/basecamp/handbook/master/getting-started.md\")\n",
        "loader.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### JSON\n",
        "\n",
        "```python\n",
        "from langchain_community.document_loaders import JSONLoader\n",
        "loader = JSONLoader(\n",
        "    file_path=\"\",\n",
        "    jq_schema='.messages[].content',\n",
        "    text_content=False)\n",
        "\n",
        "data = loader.load()\n",
        "```\n",
        "\n",
        "For other documents, the documentation of [Langchain - Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) can be referred to.\n",
        "\n",
        "## Document Splitting\n",
        "\n",
        "Splitting the text of a document in LLM (Deep Learning Language Models) can be advantageous for several reasons. Firstly, it helps manage long documents, as LLMs may struggle with processing very large texts due to memory constraints or computational limitations. It improves contextual representation by capturing local contextual relationships more effectively. \n",
        "\n",
        "All methods of `langchain.text_splitter` have the following parameters\n",
        "\n",
        "- `separator=\"\\n\"`: Character used as a separator between parts of the text (e.g., line breaks).\n",
        "- `chunk_size=100`: Maximum size of each text fragment.\n",
        "- `chunk_overlap=20`: Overlap of characters between consecutive fragments.\n",
        "- `length_function`: A function that may dynamically adjust the fragment size, though its specific function is unclear without further context.\n",
        "\n",
        "\n",
        "\n",
        "### Split by character\n",
        "\n",
        "Splits text based on a user defined character. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "markdown = WebBaseLoader(\"https://raw.githubusercontent.com/basecamp/handbook/master/how-we-work.md\")\n",
        "markdown_doc = markdown.load()\n",
        "text_markdown = markdown_doc[0].page_content\n",
        "print(text_markdown[:500])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "text_splitted = text_splitter.split_text(text_markdown[0].page_content)\n",
        "print(text_splitted[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split for markdown\n",
        "\n",
        "Splits text based on Markdown-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the Markdown)\n",
        "\n",
        "We can use `MarkdownHeaderTextSplitter` to preserve header metadata in our chunks, as show below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "]\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "md_header_splits = markdown_splitter.split_text(text_markdown[0].page_content)\n",
        "md_header_splits[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split for Code\n",
        "\n",
        "Splits text based on characters specific to coding languages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.text_splitter import (\n",
        "    Language,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "PYTHON_CODE = \"\"\"\n",
        "def hello_world():\n",
        "    print(\"Hello, World!\")\n",
        "\n",
        "# Call the function\n",
        "hello_world()\n",
        "\"\"\"\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
        ")\n",
        "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
        "python_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding and Vectorstores (Storage)\n",
        "\n",
        "### Embeddings\n",
        "\n",
        "Embeddings are vector representations of words in a dimensional space, learned during training. They capture semantic and contextual meaning to facilitate the model's understanding and processing of the text.\n",
        "\n",
        "The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).\n",
        "\n",
        "**Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embedding = OpenAIEmbeddings()\n",
        "embeddings = embedding.embed_documents(\n",
        "    [\n",
        "        \"Hi there!\",\n",
        "        \"Hello\"\n",
        "    ]\n",
        ")\n",
        "len(embeddings), len(embeddings[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code snippet demonstrates how to use the `OpenAIEmbeddings` class from LangChain to embed multiple documents. It initializes an instance of the class, embeds the provided documents, and prints the length of the embeddings along with a sample of the first embedding vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "text_embedding = embeddings[0]\n",
        "print(\"length: \", len(text_embedding), \"\\nvector_sample: \" ,text_embedding[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code snippet further explores the embeddings obtained in the previous example. It prints the length of the first embedding vector and a sample of its elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = PyPDFLoader(\"https://arxiv.org/pdf/2103.15348.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1500,\n",
        "    chunk_overlap = 150\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(pages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly, this code snippet demonstrates how to use the `RecursiveCharacterTextSplitter` class from LangChain to split documents into smaller chunks. It loads a PDF document from a URL, splits it into chunks, and stores the resulting chunks in the `splits` variable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Vectorstores\n",
        "\n",
        "To build our database, we need an array of [Documents].\n",
        "\n",
        "With Chroma, this will be done locally. Note that there is no directory referencing our Chroma database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll create the vectorstore considering the document split, embedding method, and the location of the vectorstore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "persist_directory = './vector_db_chroma/'\n",
        "\n",
        "!rm -rf ./docs/chroma  # remove old database files if any\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "print(vectordb._collection.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Searches conducted in the Chroma database will yield pieces of information that match the \"intention\" of the question, which will subsequently serve for response generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "query_1 = vectordb.similarity_search(\n",
        "    \"What are some of the challenges hindering the widespread adoption and reuse of innovations in document image analysis (DIA), particularly in comparison to disciplines like natural language processing and computer vision?\",\n",
        "    k=3,\n",
        ")\n",
        "query_2 = vectordb.similarity_search(\n",
        "    \"How does the LayoutParser library address the challenges mentioned in the summary and contribute to streamlining the usage of deep learning in DIA research and applications?\",\n",
        "    k=3,\n",
        ")\n",
        "# print(query_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "query_1[0].page_content[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieval\n",
        "\n",
        "A vector store retriever utilizes a vector store for document retrieval. It acts as a simplified interface to the vector store class, enabling compatibility with the retriever interface. This retriever leverages search functionalities provided by the vector store, such as similarity search and MMR, to retrieve texts stored within it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "question = \"What are some of the challenges hindering the widespread adoption and reuse of innovations in document image analysis (DIA), particularly in comparison to disciplines like natural language processing and computer vision?\"\n",
        "retriever = vectordb.as_retriever()\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Maximum Marginal Relevance Retrieval\n",
        "\n",
        "By default, the vector store retriever uses similarity search. If the underlying vector store supports maximum marginal relevance (`mmr`) search, you can specify that as the search type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "retriever = vectordb.as_retriever(search_type=\"mmr\")\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Similarity Score Threshold Retrieval\n",
        "\n",
        "Sets a similarity score threshold and only returns documents with a score above that threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5}\n",
        ")\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- retrieval -->\n",
        "\n",
        "## Question Answering\n",
        "\n",
        "Let's review the model setup. We have our vectorstore, we ask a question, and the vectorstore returns relevant elements to answer the question. Since these are parts of the document, they need to be passed through an LLM engine to structure (`chain`) a coherent response. Typically, within these models, there exists a parameter called `temperature` where 0 is the most precise and 1 makes the model \"ultra creative.\" This final step can be done in various ways.\n",
        "\n",
        "- `stuff`: Prepares and organizes input data or parameters.\n",
        "- `map_reduce`: Distributes computation tasks across multiple nodes or processes, often used for parallel processing and aggregating results.\n",
        "- `refine`: Improves the quality or accuracy of output by iteratively adjusting parameters or fine-tuning the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA as RQa\n",
        "\n",
        "llm_model = \"gpt-3.5-turbo\"\n",
        "llm = ChatOpenAI(model_name=llm_model, temperature=0)\n",
        "question = \"What are some of the challenges hindering the widespread adoption and reuse of innovations in document image analysis (DIA), particularly in comparison to disciplines like natural language processing and computer vision?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "stuff = RQa.from_chain_type(\n",
        "    llm, retriever=vectordb.as_retriever(),\n",
        "    chain_type=\"stuff\" # default\n",
        ")\n",
        "stuff_result = stuff({\"query\": question})\n",
        "stuff_result['result']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Map Reduce"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "m_p = RQa.from_chain_type(\n",
        "    llm, retriever=vectordb.as_retriever(),\n",
        "    chain_type=\"map_reduce\"\n",
        ")\n",
        "mp_result = m_p({\"query\": question})\n",
        "mp_result['result']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Refine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "refine = RQa.from_chain_type(\n",
        "    llm, retriever=vectordb.as_retriever(),\n",
        "    chain_type=\"refine\" \n",
        ")\n",
        "refine_result = refine({\"query\": question})\n",
        "refine_result['result']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question Answering With Prompt\n",
        "\n",
        "Now, a [prompt template](https://python.langchain.com/docs/modules/model_io/prompts/quick_start#prompttemplate) will be created to guide us in answering the question, instructing the model on how to use the provided context to generate concise answers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Build prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
        ")\n",
        "promt_result = qa_chain({\"query\": question})\n",
        "promt_result[\"result\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chat\n",
        "\n",
        "After completing the previous step, we have all the relevant information for the natural language model to interpret and provide a response considering the context and the question.\n",
        "\n",
        "To summarize, first, we need to set up the environment by providing the API key for OpenAI and adding it to our virtual environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import getpass, openai, os\n",
        "api_key = getpass.getpass(prompt=\"OPENAI - API-KEY: \")\n",
        "openai.apikey = api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we load the document, in this case, we'll use a thesis from PUCP (https://tesis.pucp.edu.pe/repositorio/handle/20.500.12404/27052)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "url_pdf = \"https://tesis.pucp.edu.pe/repositorio/bitstream/handle/20.500.12404/27040/HUAMAN%c3%8d_LLAMOCCA_ROGER_ANGEL_DESARROLLO_COMPETENCIAS.pdf?sequence=1&isAllowed=y\"\n",
        "loader = PyPDFLoader(url_pdf)\n",
        "pages = loader.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we define the splitting module to generate text chunks with the previously loaded `Document`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1500,\n",
        "    chunk_overlap = 150\n",
        ")\n",
        "splits = text_splitter.split_documents(pages)\n",
        "len(splits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we generate the vector database (thesis) using the OPENAI embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "persist_directory = './thesis_chroma/'\n",
        "\n",
        "!rm -rf ./thesis_chroma  # remove old database files if any (linux, Mac)\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to retrieve the previously created vectorbase, you only need to locate the directory of the database and pass the embedding method used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function = embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA as RQa\n",
        "\n",
        "llm_model = \"gpt-3.5-turbo\"\n",
        "llm = ChatOpenAI(model_name = llm_model, temperature = 0)\n",
        "question = input(\"Enter the question to answer: \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "stuff = RQa.from_chain_type(\n",
        "    llm, retriever = vectordb.as_retriever(),\n",
        "    chain_type = \"stuff\" # default\n",
        ")\n",
        "stuff = result = stuff({\"query\": question})\n",
        "stuff = result['result']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Result\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import getpass, openai, os\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA as RQa\n",
        "\n",
        "api_key = getpass.getpass(prompt=\"Insert your OPENAI - API-KEY: \")\n",
        "openai.apikey = api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "\n",
        "url_pdf = input(\"Insert the pdfurl: \")\n",
        "\n",
        "loader = PyPDFLoader(url_pdf)\n",
        "pages = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1500,\n",
        "    chunk_overlap = 150\n",
        ")\n",
        "splits = text_splitter.split_documents(pages)\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "persist_directory = './thesis_chroma/'\n",
        "\n",
        "!rm -rf ./thesis_chroma  # remove old database files if any (linux, Mac)\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "\n",
        "llm_model = \"gpt-3.5-turbo\"\n",
        "llm = ChatOpenAI(model_name = llm_model, temperature = 0)\n",
        "while True:\n",
        "    question = input(\"Ingrese la pregunta a contestar: \")\n",
        "    if question == \"\":\n",
        "        break\n",
        "    stuff = RQa.from_chain_type(\n",
        "        llm, retriever = vectordb.as_retriever(),\n",
        "        chain_type = \"stuff\" # default\n",
        "    )\n",
        "    stuff_result = stuff({\"query\": question})\n",
        "    result = stuff_result['result']\n",
        "    print(result, end='')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}