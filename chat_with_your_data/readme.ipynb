{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chat With Your Data\n",
        "\n",
        "Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install langchain openai pypdf python-dotenv chromadb lark -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estructura \n",
        "\n",
        "- API keys y Variables de entorno \n",
        "- Document Loading\n",
        "- Document Splitting\n",
        "- Vectorstores and Embedding (Storage)\n",
        "- Retrieval\n",
        "- Question Answering\n",
        "- Chat\n",
        "- Conclusion\n",
        "\n",
        "<!-- ![](figs/0_preview.png) -->\n",
        "![](https://python.langchain.com/assets/images/data_connection-95ff2033a8faa5f3ba41376c0f6dd32a.jpg)\n",
        "\n",
        "In this tutorial, we will delve into the essential steps for creating a natural language model. We'll begin by understanding the significance of API keys and utilizing environment variables to ensure security and privacy in our applications. Next, we'll dive into document loading, mastering the handling of various file types and data sources. Following that, we'll tackle document splitting for efficient processing. We'll then proceed to create vector stores and embeddings, crucial for representing the semantic meaning of words. Afterward, we'll explore information retrieval techniques and question-answering capabilities, culminating in the implementation of a chat system based on our model. \n",
        "\n",
        "Finally, we'll draw conclusions on the challenges and possibilities within this fascinating field of natural language processing.\n",
        "\n",
        "\n",
        "## Api Key y Environment VAriables\n",
        "\n",
        "Crear la API de OPENAI, https://platform.openai.com/account/api-keys\n",
        "\n",
        "Guardar esa api\n",
        "\n",
        "Para cargar esa API existen varias maneras, una es utilizar archivos de entorno lo cuales \"deben\" ser privados, es decir que deben incluirse en el `.gitignore` si se esta trabajando en un entorno de trabajo, (actualmente github detecta si se subio alguna llave a la plataforma, soltara una alerta e inhabilitara la apikey teniendo que crear otra). Y colocarlo manualmente .\n",
        "\n",
        "### `Dotenv`\n",
        "\n",
        "Para este metodo se puede crear un archivo `.env` en el entorno de trabajo y enumerar las variables de la siguiente manera `variable = \"value_variable\"`\n",
        "\n",
        "\n",
        "```.env\n",
        "NAME_OF_VARIABLE=\"sk-xxxxxxxxxxxxxxxx\"\n",
        "```\n",
        "\n",
        "Y para utilizar se usa `python-dotenv`, el cual mediante las funciones `load_dotenv` y `find_dotenv`, cargara las variables dentro del archivo `.env`. \n",
        "\n",
        "```python\n",
        "# `!pip install python-dotenv`\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "secret_variable = os.environ['NAME_OF_VARIABLE']\n",
        "```\n",
        "\n",
        "### Colab \n",
        "\n",
        "Para colab podemos introducir un formulario con getpass, el cual ocultara la apikey cuando es introducida, la desventaja a comparacion del anterior metodo es que siempre tendremos que pegar la api key cada vez que se ejecute el archivo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# !pip install openai\n",
        "import getpass, openai, os\n",
        "api_key = getpass.getpass(prompt=\"OPENAI - KEY: \")\n",
        "openai.apikey = api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document Loading\n",
        "\n",
        "Se debe considerar 2 casos, si se extraera la informacion de la web o esta en nuestro entorno de trabajo. Para el primer caso es posible (aunque `langchain` ya tiene implementado estos casos) que necesitemos de la libreria `requests` para poder descargar el archivo o el contenido del archivo. Mientras que para el segundo caso solo basta la ruta relativa o absoluta del archivo. \n",
        "\n",
        "Para todos los documentos se tiene esta estructura, el cual retornara una lista de objetos de tipo `Document` el cual tiene 2 objetos dentro, uno es el `page_content` el cual es el texto dentro, y la `metadata`.\n",
        "\n",
        "```\n",
        "from langchain.document_loaders import `Method``\n",
        "file = Method(file_path)\n",
        "file_read = file.lod()\n",
        "print(file_read[0])\n",
        "Document(\n",
        "    page_content: \"text\",\n",
        "    metadata: {\"source\": file_path, ...}\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "### PDFs\n",
        "\n",
        "Para archivo pdf ya se tiene implementado para poder leer el documento por la url y path local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"https://arxiv.org/pdf/2103.15348.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "print(pages[0].page_content(:100))\n",
        "print(pages[0].metadata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Web Plain Text\n",
        "\n",
        "Para textos planos de la url, se puede utilizar el `WebBaseLoader`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://raw.githubusercontent.com/basecamp/handbook/master/getting-started.md\")\n",
        "loader.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# from langchain_community.document_loaders import JSONLoader\n",
        "# loader = JSONLoader(\n",
        "#     file_path=\"\",\n",
        "#     jq_schema='.messages[].content',\n",
        "#     text_content=False)\n",
        "\n",
        "# data = loader.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para otros documentos se puede leer la documentacion de [Langchain - Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n",
        "\n",
        "## Document Splitting\n",
        "\n",
        "Splitting the text of a document in LLM (Deep Learning Language Models) can be advantageous for several reasons. Firstly, it helps manage long documents, as LLMs may struggle with processing very large texts due to memory constraints or computational limitations. It improves contextual representation by capturing local contextual relationships more effectively. \n",
        "\n",
        "All methods of `langchain.text_splitter` have the following parameters\n",
        "\n",
        "- `separator=\"\\n\"`: Character used as a separator between parts of the text (e.g., line breaks).\n",
        "- `chunk_size=100`: Maximum size of each text fragment.\n",
        "- `chunk_overlap=20`: Overlap of characters between consecutive fragments.\n",
        "- `length_function`: A function that may dynamically adjust the fragment size, though its specific function is unclear without further context.\n",
        "\n",
        "\n",
        "\n",
        "### Split by character\n",
        "\n",
        "Splits text based on a user defined character. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "markdown = WebBaseLoader(\"https://raw.githubusercontent.com/basecamp/handbook/master/how-we-work.md\")\n",
        "markdown_doc = markdown.load()\n",
        "text_markdown = markdown_doc[0].page_content\n",
        "print(text_markdown[:500])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "text_splitted = text_splitter.split_text(text_markdown[0].page_content)\n",
        "print(text_splitted[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split for markdown\n",
        "\n",
        "Splits text based on Markdown-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the Markdown)\n",
        "\n",
        "We can use `MarkdownHeaderTextSplitter` to preserve header metadata in our chunks, as show below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "]\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "md_header_splits = markdown_splitter.split_text(text_markdown[0].page_content)\n",
        "md_header_splits[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split for Code\n",
        "\n",
        "Splits text based on characters specific to coding languages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.text_splitter import (\n",
        "    Language,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "PYTHON_CODE = \"\"\"\n",
        "def hello_world():\n",
        "    print(\"Hello, World!\")\n",
        "\n",
        "# Call the function\n",
        "hello_world()\n",
        "\"\"\"\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
        ")\n",
        "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
        "python_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding and Vectorstores (Storage)\n",
        "\n",
        "### Embeddings\n",
        "\n",
        "Embeddings are vector representations of words in a dimensional space, learned during training. They capture semantic and contextual meaning to facilitate the model's understanding and processing of the text.\n",
        "\n",
        "The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).\n",
        "\n",
        "\n",
        "Example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embedding = OpenAIEmbeddings()\n",
        "embeddings = embedding.embed_documents(\n",
        "    [\n",
        "        \"Hi there!\",\n",
        "        \"Hello\"\n",
        "    ]\n",
        ")\n",
        "len(embeddings), len(embeddings[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "text_embedding = embeddings[0]\n",
        "print(\"length: \", len(text_embedding), \"\\nvector_sample: \" ,text_embedding[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = PyPDFLoader(\"https://arxiv.org/pdf/2103.15348.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1500,\n",
        "    chunk_overlap = 150\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(pages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vectorstores\n",
        "\n",
        "Para poder \n",
        "Para nuestro base de datos, debemos tener un array de [Documents].\n",
        "\n",
        "Con Chroma se hara de manera local, note que no hay ningun directorio que haga referencia al nuestra base de datos de Chroma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora se creara el vectorstore tomando en cuenta el split del documento, el metodo de embedding y la ubicacion de vectorstore "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "persist_directory = './vector_db_chroma/'\n",
        "\n",
        "!rm -rf ./docs/chroma  # remove old database files if any\n",
        "\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "print(vectordb._collection.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarity search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "query_1 = vectordb.similarity_search(\n",
        "    \"What are some of the challenges hindering the widespread adoption and reuse of innovations in document image analysis (DIA), particularly in comparison to disciplines like natural language processing and computer vision?\",\n",
        "    k=3,\n",
        ")\n",
        "query_2 = vectordb.similarity_search(\n",
        "    \"How does the LayoutParser library address the challenges mentioned in the summary and contribute to streamlining the usage of deep learning in DIA research and applications?\",\n",
        "    k=3,\n",
        ")\n",
        "# print(query_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "query_1[0].page_content[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieval\n",
        "## Question Answering\n",
        "## Chat\n",
        "## Conclusion"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}